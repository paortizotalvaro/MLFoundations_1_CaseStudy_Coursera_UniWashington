{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Week 2: Multiple Regression (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first notebook we explored multiple regression using Turi Create. Now we will use Turi Create along with numpy to solve for the regression weights with gradient descent.\n",
    "\n",
    "In this notebook we will cover estimating multiple regression weights via gradient descent. You will:\n",
    "* Add a constant column of 1's to a Turi Create SFrame to account for the intercept\n",
    "* Convert an SFrame into a Numpy array\n",
    "* Write a predict_output() function using Numpy\n",
    "* Write a numpy function to compute the derivative of the regression weights with respect to a single feature\n",
    "* Write gradient descent function to compute the regression weights given an initial weight vector, step size and tolerance.\n",
    "* Use the gradient descent function to estimate regression weights for multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire up Turi Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of Turi Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate\n",
    "\n",
    "from math import sqrt # recall that the magnitude/length of a vector [g[0], g[1], g[2]] is sqrt(g[0]^2 + g[1]^2 + g[2]^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in house sales data\n",
    "\n",
    "Dataset is from house sales in King County, the region where the city of Seattle, WA is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = turicreate.SFrame('../data/home_data.sframe/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do any \"feature engineering\" like creating new features or adjusting existing ones we should do this directly using the SFrames as seen in the other Week 2 notebook. For this notebook, however, we will work with the existing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">date</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">price</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">bedrooms</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">bathrooms</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">sqft_living</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">sqft_lot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">floors</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">waterfront</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7129300520</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2014-10-13 00:00:00+00:00</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">221900.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1180.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5650.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6414100192</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2014-12-09 00:00:00+00:00</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">538000.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.25</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2570.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7242.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5631500400</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2015-02-25 00:00:00+00:00</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">180000.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">770.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10000.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2487200875</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2014-12-09 00:00:00+00:00</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">604000.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1960.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5000.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1954400510</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2015-02-18 00:00:00+00:00</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">510000.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1680.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8080.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7237550310</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2014-05-12 00:00:00+00:00</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1225000.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5420.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">101930.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1321400060</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2014-06-27 00:00:00+00:00</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">257500.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.25</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1715.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6819.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2008000270</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2015-01-15 00:00:00+00:00</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">291850.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1060.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9711.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2414600126</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2015-04-15 00:00:00+00:00</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">229500.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1780.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7470.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3793500160</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2015-03-12 00:00:00+00:00</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">323000.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1890.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6560.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">view</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">condition</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">grade</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">sqft_above</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">sqft_basement</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">yr_built</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">yr_renovated</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">zipcode</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">lat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1180.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1955.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">98178</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">47.51123398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2170.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">400.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1951.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1991.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">98125</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">47.72102274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">770.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1933.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">98028</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">47.73792661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1050.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">910.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1965.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">98136</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">47.52082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1680.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1987.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">98074</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">47.61681228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3890.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1530.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2001.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">98053</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">47.65611835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1715.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1995.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">98003</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">47.30972002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1060.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1963.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">98198</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">47.40949984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1050.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">730.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1960.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">98146</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">47.51229381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1890.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2003.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">98038</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">47.36840673</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">long</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">sqft_living15</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">sqft_lot15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-122.25677536</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1340.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-122.3188624</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1690.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-122.23319601</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2720.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8062.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-122.39318505</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1360.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-122.04490059</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1800.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-122.00528655</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4760.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">101930.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-122.32704857</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2238.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-122.31457273</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1650.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-122.33659507</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1780.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-122.0308176</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2390.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7570.0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 21 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tid\tstr\n",
       "\tdate\tdatetime\n",
       "\tprice\tfloat\n",
       "\tbedrooms\tfloat\n",
       "\tbathrooms\tfloat\n",
       "\tsqft_living\tfloat\n",
       "\tsqft_lot\tfloat\n",
       "\tfloors\tfloat\n",
       "\twaterfront\tint\n",
       "\tview\tint\n",
       "\tcondition\tint\n",
       "\tgrade\tfloat\n",
       "\tsqft_above\tfloat\n",
       "\tsqft_basement\tfloat\n",
       "\tyr_built\tfloat\n",
       "\tyr_renovated\tfloat\n",
       "\tzipcode\tstr\n",
       "\tlat\tfloat\n",
       "\tlong\tfloat\n",
       "\tsqft_living15\tfloat\n",
       "\tsqft_lot15\tfloat\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+------------+---------------------------+-----------+----------+-----------+\n",
       "|     id     |            date           |   price   | bedrooms | bathrooms |\n",
       "+------------+---------------------------+-----------+----------+-----------+\n",
       "| 7129300520 | 2014-10-13 00:00:00+00:00 |  221900.0 |   3.0    |    1.0    |\n",
       "| 6414100192 | 2014-12-09 00:00:00+00:00 |  538000.0 |   3.0    |    2.25   |\n",
       "| 5631500400 | 2015-02-25 00:00:00+00:00 |  180000.0 |   2.0    |    1.0    |\n",
       "| 2487200875 | 2014-12-09 00:00:00+00:00 |  604000.0 |   4.0    |    3.0    |\n",
       "| 1954400510 | 2015-02-18 00:00:00+00:00 |  510000.0 |   3.0    |    2.0    |\n",
       "| 7237550310 | 2014-05-12 00:00:00+00:00 | 1225000.0 |   4.0    |    4.5    |\n",
       "| 1321400060 | 2014-06-27 00:00:00+00:00 |  257500.0 |   3.0    |    2.25   |\n",
       "| 2008000270 | 2015-01-15 00:00:00+00:00 |  291850.0 |   3.0    |    1.5    |\n",
       "| 2414600126 | 2015-04-15 00:00:00+00:00 |  229500.0 |   3.0    |    1.0    |\n",
       "| 3793500160 | 2015-03-12 00:00:00+00:00 |  323000.0 |   3.0    |    2.5    |\n",
       "+------------+---------------------------+-----------+----------+-----------+\n",
       "+-------------+----------+--------+------------+------+-----------+-------+\n",
       "| sqft_living | sqft_lot | floors | waterfront | view | condition | grade |\n",
       "+-------------+----------+--------+------------+------+-----------+-------+\n",
       "|    1180.0   |  5650.0  |  1.0   |     0      |  0   |     3     |  7.0  |\n",
       "|    2570.0   |  7242.0  |  2.0   |     0      |  0   |     3     |  7.0  |\n",
       "|    770.0    | 10000.0  |  1.0   |     0      |  0   |     3     |  6.0  |\n",
       "|    1960.0   |  5000.0  |  1.0   |     0      |  0   |     5     |  7.0  |\n",
       "|    1680.0   |  8080.0  |  1.0   |     0      |  0   |     3     |  8.0  |\n",
       "|    5420.0   | 101930.0 |  1.0   |     0      |  0   |     3     |  11.0 |\n",
       "|    1715.0   |  6819.0  |  2.0   |     0      |  0   |     3     |  7.0  |\n",
       "|    1060.0   |  9711.0  |  1.0   |     0      |  0   |     3     |  7.0  |\n",
       "|    1780.0   |  7470.0  |  1.0   |     0      |  0   |     3     |  7.0  |\n",
       "|    1890.0   |  6560.0  |  2.0   |     0      |  0   |     3     |  7.0  |\n",
       "+-------------+----------+--------+------------+------+-----------+-------+\n",
       "+------------+---------------+----------+--------------+---------+-------------+\n",
       "| sqft_above | sqft_basement | yr_built | yr_renovated | zipcode |     lat     |\n",
       "+------------+---------------+----------+--------------+---------+-------------+\n",
       "|   1180.0   |      0.0      |  1955.0  |     0.0      |  98178  | 47.51123398 |\n",
       "|   2170.0   |     400.0     |  1951.0  |    1991.0    |  98125  | 47.72102274 |\n",
       "|   770.0    |      0.0      |  1933.0  |     0.0      |  98028  | 47.73792661 |\n",
       "|   1050.0   |     910.0     |  1965.0  |     0.0      |  98136  |   47.52082  |\n",
       "|   1680.0   |      0.0      |  1987.0  |     0.0      |  98074  | 47.61681228 |\n",
       "|   3890.0   |     1530.0    |  2001.0  |     0.0      |  98053  | 47.65611835 |\n",
       "|   1715.0   |      0.0      |  1995.0  |     0.0      |  98003  | 47.30972002 |\n",
       "|   1060.0   |      0.0      |  1963.0  |     0.0      |  98198  | 47.40949984 |\n",
       "|   1050.0   |     730.0     |  1960.0  |     0.0      |  98146  | 47.51229381 |\n",
       "|   1890.0   |      0.0      |  2003.0  |     0.0      |  98038  | 47.36840673 |\n",
       "+------------+---------------+----------+--------------+---------+-------------+\n",
       "+---------------+---------------+-----+\n",
       "|      long     | sqft_living15 | ... |\n",
       "+---------------+---------------+-----+\n",
       "| -122.25677536 |     1340.0    | ... |\n",
       "|  -122.3188624 |     1690.0    | ... |\n",
       "| -122.23319601 |     2720.0    | ... |\n",
       "| -122.39318505 |     1360.0    | ... |\n",
       "| -122.04490059 |     1800.0    | ... |\n",
       "| -122.00528655 |     4760.0    | ... |\n",
       "| -122.32704857 |     2238.0    | ... |\n",
       "| -122.31457273 |     1650.0    | ... |\n",
       "| -122.33659507 |     1780.0    | ... |\n",
       "|  -122.0308176 |     2390.0    | ... |\n",
       "+---------------+---------------+-----+\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Numpy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although SFrames offer a number of benefits to users (especially when using Big Data and built-in Turi Create functions) in order to understand the details of the implementation of algorithms it's important to work with a library that allows for direct (and optimized) matrix operations. Numpy is a Python solution to work with matrices (or any multi-dimensional \"array\").\n",
    "\n",
    "Recall that the predicted value given the weights and the features is just the dot product between the feature and weight vector. Similarly, if we put all of the features row-by-row in a matrix then the predicted value for *all* the observations can be computed by right multiplying the \"feature matrix\" by the \"weight vector\". \n",
    "\n",
    "First we need to take the SFrame of our data and convert it into a 2D numpy array (also called a matrix). To do this we use Turi Create's built in .to_dataframe() which converts the SFrame into a Pandas (another python library) dataframe. We can then use Panda's .as_matrix() to convert the dataframe into a numpy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # note this allows us to refer to numpy as np instead "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function that will accept an SFrame, a list of feature names (e.g. ['sqft_living', 'bedrooms']) and an target feature e.g. ('price') and will return two things:\n",
    "* A numpy matrix whose columns are the desired features plus a constant column (this is how we create an 'intercept')\n",
    "* A numpy array containing the values of the output\n",
    "\n",
    "With this in mind, complete the following function (where there's an empty line you should write a line of code that does what the comment above indicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(data_sframe, features, output):\n",
    "    data_sframe['constant'] = 1 # this is how you add a constant column to an SFrame\n",
    "    # add the column 'constant' to the front of the features list so that we can extract it along with the others:\n",
    "    features = ['constant'] + features # this is how you combine two lists\n",
    "    # select the columns of data_SFrame given by the features list into the SFrame features_sframe (now including constant):\n",
    "    features_sframe = data_sframe[features]\n",
    "    # the following line will convert the features_SFrame into a numpy matrix:\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    # assign the column of data_sframe associated with the output to the SArray output_sarray\n",
    "    output_sarray = data_sframe[output]\n",
    "    # the following will convert the SArray into a numpy array by first converting it to a list\n",
    "    output_array = output_sarray.to_numpy()\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing let's use the 'sqft_living' feature and a constant as our features and price as our output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00e+00 1.18e+03]\n",
      "221900.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') # the [] around 'sqft_living' makes it a list\n",
    "print(example_features[0,:]) # this accesses the first row of the data the ':' indicates 'all columns'\n",
    "print(example_output[0]) # and the corresponding output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting output given regression weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we had the weights [1.0, 1.0] and the features [1.0, 1180.0] and we wanted to compute the predicted output 1.0\\*1.0 + 1.0\\*1180.0 = 1181.0 this is the dot product between these two arrays. If they're numpy arrayws we can use np.dot() to compute this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product:\n",
      " [1. 1.] . [1.00e+00 1.18e+03]  = 1181.0\n"
     ]
    }
   ],
   "source": [
    "# example_features:numpy matrix, and example_output:numpy array\n",
    "my_weights = np.array([1., 1.]) # the example weights\n",
    "my_features = example_features[0,] # we'll use the first data point\n",
    "\n",
    "\n",
    "predicted_value = np.dot(my_features, my_weights)\n",
    "print('dot product:\\n', my_weights, '.', my_features, ' =', predicted_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.dot() also works when dealing with a matrix and a vector. Recall that the predictions from all the observations is just the RIGHT (as in weights on the right) dot product between the features *matrix* and the weights *vector*. With this in mind finish the following predict_output function to compute the predictions for an entire matrix of features given the matrix and the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    # assume feature_matrix is a numpy matrix containing the features as columns and weights is a corresponding numpy array\n",
    "    # create the predictions vector by using np.dot()\n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your code run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181.0\n",
      "2571.0\n"
     ]
    }
   ],
   "source": [
    "test_predictions = predict_output(example_features, my_weights)\n",
    "print(test_predictions[0]) # should be 1181.0\n",
    "print(test_predictions[1]) # should be 2571.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [1.00e+00 1.18e+03] . [1. 1.]  = 1181.0\n",
      "1: [1.00e+00 2.57e+03] . [1. 1.]  = 2571.0\n",
      "10: [1.00e+00 3.56e+03] . [1. 1.]  = 3561.0\n"
     ]
    }
   ],
   "source": [
    "print('0:', example_features[0], '.', my_weights, ' =', test_predictions[0])\n",
    "print('1:', example_features[1], '.', my_weights, ' =', test_predictions[1])\n",
    "print('10:', example_features[10], '.', my_weights, ' =', test_predictions[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to move to computing the derivative of the regression cost function. Recall that the cost function is the sum over the data points of the squared difference between an observed output and a predicted output.\n",
    "\n",
    "Since the derivative of a sum is the sum of the derivatives we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:\n",
    "\n",
    "(w[0]\\*[CONSTANT] + w[1]\\*[feature_1] + ... + w[i] \\*[feature_i] + ... +  w[k]\\*[feature_k] - output)^2\n",
    "\n",
    "Where we have k features and a constant. So the derivative with respect to weight w[i] by the chain rule is:\n",
    "\n",
    "2\\*(w[0]\\*[CONSTANT] + w[1]\\*[feature_1] + ... + w[i] \\*[feature_i] + ... +  w[k]\\*[feature_k] - output)\\* [feature_i]\n",
    "\n",
    "The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:\n",
    "\n",
    "2\\*error\\*[feature_i]\n",
    "\n",
    "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!\n",
    "\n",
    "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors. \n",
    "\n",
    "With this in mind complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    # Assume that errors and feature are both numpy arrays of the same length (number of data points)\n",
    "    # compute twice the dot product of these vectors as 'derivative' and return the value\n",
    "    derivative = 2*np.dot(feature,errors)\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your feature derivartive run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my weights are all 0, this makes all the predictions 0. For example:\n",
      "test predictions0: [1.00e+00 1.18e+03] . [0. 0.]  = 0.0\n",
      "test predictions1: [1.00e+00 2.57e+03] . [0. 0.]  = 0.0\n",
      "test predictions10: [1.00e+00 3.56e+03] . [0. 0.]  = 0.0\n",
      "\n",
      " Prediction errors in this case is just the example_output: \n",
      "output0: 221900.0 errors0 -221900.0\n",
      "output1: 538000.0 errors1 -538000.0\n",
      "output10: 662500.0 errors10 -662500.0\n",
      "\n",
      "derivative with respect to w_0\n",
      "-23345850022.0\n",
      "-23345850022.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- 1. Get predictions based on example_features and example_output ---------\n",
    "# example_features:numpy matrix, and example_output:numpy array, obtained above with: \n",
    "# (example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') \n",
    "my_weights = np.array([0., 0.]) # this makes all the predictions 0\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "\n",
    "print('my weights are all 0, this makes all the predictions 0. For example:')\n",
    "print('test predictions0:', example_features[0], '.', my_weights, ' =', test_predictions[0])\n",
    "print('test predictions1:', example_features[1], '.', my_weights, ' =', test_predictions[1])\n",
    "print('test predictions10:', example_features[10], '.', my_weights, ' =', test_predictions[10])\n",
    "\n",
    "# -------- 2. Get the errors ---------\n",
    "# just like SFrames 2 numpy arrays can be elementwise subtracted with '-': \n",
    "errors = test_predictions - example_output \n",
    "print('\\n Prediction errors in this case is just the example_output: ')\n",
    "print('output0:',example_output[0],'errors0',errors[0])\n",
    "print('output1:',example_output[1],'errors1',errors[1])\n",
    "print('output10:',example_output[10],'errors10',errors[10])\n",
    "\n",
    "\n",
    "# -------- 2. Get the derivative ---------\n",
    "feature = example_features[:,0] # let's compute the derivative with respect to 'constant', the \":\" indicates \"all rows\"\n",
    "derivative = feature_derivative(errors, feature)\n",
    "print('\\nderivative with respect to w_0')\n",
    "print(derivative)\n",
    "print(-np.sum(example_output)*2) # should be the same as derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of *increase* and therefore the negative gradient is the direction of *decrease* and we're trying to *minimize* a cost function. \n",
    "\n",
    "The amount by which we move in the negative gradient *direction*  is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. We define this by requiring the magnitude (length) of the gradient vector to be smaller than a fixed 'tolerance'.\n",
    "\n",
    "With this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent we update the weight for each feature befofe computing our stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    while not converged:\n",
    "        # compute the predictions based on feature_matrix and weights using your predict_output() function\n",
    "        #predictions = predict_output(feature_matrix, weights)\n",
    "        predictions = np.dot(feature_matrix, weights)\n",
    "        print('\\n predictions', predictions)\n",
    "        \n",
    "        # compute the errors as predictions - output\n",
    "        errors = predictions - output\n",
    "        print('errors', errors)\n",
    "        \n",
    "        gradient_sum_squares = 0 # initialize the gradient sum of squares\n",
    "        \n",
    "        # **************** update each feature's weight **************\n",
    "        # while we haven't reached the tolerance yet, update each feature's weight\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Recall that feature_matrix[:, i] is the feature column associated with weights[i]\n",
    "            \n",
    "            # ******* Update the ith weight ******\n",
    "            # 1. compute the derivative for weight[i]:\n",
    "            #deriv_weight_i = feature_derivative(errors, feature=feature_matrix[:,i])\n",
    "            deriv_weight_i = 2*np.dot(feature_matrix[:,i],errors)\n",
    "            \n",
    "            # 2. subtract the step size times the derivative from the current weight\n",
    "            weights[i] -= (step_size * deriv_weight_i)\n",
    "            \n",
    "            # ******* Asses convergence ******\n",
    "            # Add the ith term for the magnitude of the gradient vector \n",
    "            #add the squared value of the derivative to the gradient sum of squares \n",
    "            gradient_sum_squares += (deriv_weight_i * deriv_weight_i)\n",
    "            \n",
    "            \n",
    "        # compute the square-root of the gradient sum of squares to get the gradient magnitude:\n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "        print('gradient', gradient_sum_squares)\n",
    "        print('gradient magnitude', gradient_magnitude)\n",
    "        \n",
    "        # determine convergence\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note before we run the gradient descent. Since the gradient is a sum over all the data points and involves a product of an error and a feature the gradient itself will be very large since the features are large (squarefeet) and the output is large (prices). So while you might expect \"tolerance\" to be small, small is only relative to the size of the features. \n",
    "\n",
    "For similar reasons the step size will be much smaller than you might expect but this is because the gradient has such large values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Gradient Descent as Simple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's split the data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = sales.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the gradient descent is designed for multiple regression since the constant is now a feature we can use the gradient descent function to estimate the parameters in the simple regression on squarefeet. The folowing cell sets up the feature_matrix, output, initial weights and step size for the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test out the gradient descent\n",
    "simple_features = ['sqft_living']\n",
    "my_output = 'price'\n",
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n",
    "\n",
    "initial_weights = np.array([-47000., 1.])\n",
    "step_size = 7e-12\n",
    "tolerance = 2.5e7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next run your gradient descent with the above parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " predictions [-45820. -44430. -46230. ... -45470. -45400. -45980.]\n",
      "errors [-267720. -582430. -226230. ... -405470. -445400. -370980.]\n",
      "gradient 2.555457263634446e+27\n",
      "gradient magnitude 50551530774393.43\n",
      "\n",
      " predictions [371735.75268253 864992.10740189 226242.87107465 ... 495936.99307949\n",
      " 520777.24115888 314958.04278677]\n",
      "errors [149835.75268253 326992.10740189  46242.87107465 ... 135936.99307949\n",
      " 120777.24115888 -10041.95721323]\n",
      "gradient 1.7232997037567737e+26\n",
      "gradient magnitude 13127451023548.988\n",
      "\n",
      " predictions [263302.97905279 628829.24562979 155485.87883943 ... 355341.9670398\n",
      " 373749.76463721 221228.01311587]\n",
      "errors [  41402.97905279   90829.24562979  -24514.12116057 ...   -4658.0329602\n",
      "  -26250.23536279 -103771.98688413]\n",
      "gradient 1.1621254290688472e+25\n",
      "gradient magnitude 3408996082527.5923\n",
      "\n",
      " predictions [291461.29400806 690157.08979222 173860.37582713 ... 391852.3217235\n",
      " 411930.52726658 245568.25276672]\n",
      "errors [ 69561.29400806 152157.08979222  -6139.62417287 ...  31852.3217235\n",
      "  11930.52726658 -79431.74723328]\n",
      "gradient 7.836916062510473e+23\n",
      "gradient magnitude 885263580099.7618\n",
      "\n",
      " predictions [284149.01493414 674231.19678206 169088.80302216 ... 382371.14705412\n",
      " 402015.57347812 239247.46882215]\n",
      "errors [ 62249.01493414 136231.19678206 -10911.19697784 ...  22371.14705412\n",
      "   2015.57347812 -85752.53117785]\n",
      "gradient 5.284907449317886e+22\n",
      "gradient magnitude 229889265719.77835\n",
      "\n",
      " predictions [286047.9007599  678366.90498524 170327.90670783 ... 384833.26153607\n",
      " 404590.3336913  240888.8786908 ]\n",
      "errors [ 64147.9007599  140366.90498524  -9672.09329217 ...  24833.26153607\n",
      "   4590.3336913  -84111.1213092 ]\n",
      "gradient 3.5639333799248543e+21\n",
      "gradient magnitude 59698688259.666595\n",
      "\n",
      " predictions [285554.78946435 677292.92550245 170006.13063297 ... 384193.88846675\n",
      " 403921.70826723 240462.6299204 ]\n",
      "errors [ 63654.78946435 139292.92550245  -9993.86936703 ...  24193.88846675\n",
      "   3921.70826723 -84537.3700796 ]\n",
      "gradient 2.403376270740017e+20\n",
      "gradient magnitude 15502826422.107735\n",
      "\n",
      " predictions [285682.84284131 677571.82137077 170089.69090096 ... 384359.92376599\n",
      " 404095.33995093 240573.32013288]\n",
      "errors [ 63782.84284131 139571.82137077  -9910.30909904 ...  24359.92376599\n",
      "   4095.33995093 -84426.67986712]\n",
      "gradient 1.6207423145187258e+19\n",
      "gradient magnitude 4025844401.5122166\n",
      "\n",
      " predictions [285649.58935592 677499.39642565 170067.99158716 ... 384316.80696341\n",
      " 404050.25048491 240544.5755925 ]\n",
      "errors [ 63749.58935592 139499.39642565  -9932.00841284 ...  24316.80696341\n",
      "   4050.25048491 -84455.4244075 ]\n",
      "gradient 1.0929651759284709e+18\n",
      "gradient magnitude 1045449748.1603173\n",
      "\n",
      " predictions [285658.22476891 677518.20406441 170073.62655945 ... 384328.00372821\n",
      " 404061.95952007 240552.0401018 ]\n",
      "errors [ 63758.22476891 139518.20406441  -9926.37344055 ...  24328.00372821\n",
      "   4061.95952007 -84447.9598982 ]\n",
      "gradient 7.37056754262461e+16\n",
      "gradient magnitude 271487891.8593721\n",
      "\n",
      " predictions [285655.98228259 677513.32001158 170072.16324022 ... 384325.09609924\n",
      " 404058.91886257 240550.10168069]\n",
      "errors [ 63755.98228259 139513.32001158  -9927.83675978 ...  24325.09609924\n",
      "   4058.91886257 -84449.89831931]\n",
      "gradient 4970830187763630.0\n",
      "gradient magnitude 70504114.68675874\n",
      "\n",
      " predictions [285656.56461854 677514.58832478 170072.54323757 ... 384325.85116328\n",
      " 404059.70847223 240550.60505524]\n",
      "errors [ 63756.56461854 139514.58832478  -9927.45676243 ...  24325.85116328\n",
      "   4059.70847223 -84449.39494476]\n",
      "gradient 335623013348862.94\n",
      "gradient magnitude 18320016.739863064\n"
     ]
    }
   ],
   "source": [
    "simple_weights = regression_gradient_descent(feature_matrix=simple_feature_matrix\n",
    "                                      , output=output\n",
    "                                      , initial_weights=initial_weights\n",
    "                                      , step_size=step_size\n",
    "                                      , tolerance=tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do your weights compare to those achieved in week 1 (don't expect them to be exactly the same)? \n",
    "\n",
    "**Quiz Question: What is the value of the weight for sqft_living -- the second element of simple_weights (rounded to 1 decimal place)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-46999.88716555,    281.91211912])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your newly estimated weights and your predict_output() function to compute the predictions on all the TEST data (you will need to create a numpy array of the test feature_matrix and test output first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_simple_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute your predictions using test_simple_feature_matrix and your weights from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict_output(feature_matrix=test_simple_feature_matrix, weights=simple_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: What is the predicted price for the 1st house in the TEST data set for model 1 (round to nearest dollar)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([356134.44317093, 784640.86422788, 435069.83652353, ...,\n",
       "       663418.65300782, 604217.10799338, 240550.4743332 ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the predictions on test data, compute the RSS on the test data set. Save this value for comparison later. Recall that RSS is the sum of the squared errors (difference between prediction and output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_sum_of_squares(predictions, output):\n",
    "    # Then compute the residuals/errors\n",
    "    residuals = predictions - output\n",
    "\n",
    "    # Then square and add them up\n",
    "    RSS = (residuals * residuals).sum()\n",
    "\n",
    "    return(RSS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275400047593155.94"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS = get_residual_sum_of_squares(predictions=test_predictions, output=test_output)\n",
    "RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a multiple regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use more than one actual feature. Use the following code to produce the weights for a second model with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features_2 = ['sqft_living', 'sqft_living15'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "my_output = 'price'\n",
    "(feature_matrix_2, output_2) = get_numpy_data(train_data, model_features_2, my_output)\n",
    "\n",
    "initial_weights_2 = np.array([-100000., 1., 1.])\n",
    "step_size_2 = 4e-12\n",
    "tolerance_2 = 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above parameters to estimate the model weights. Record these values for your quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " predictions [-97480. -95740. -96510. ... -96940. -96990. -97960.]\n",
      "errors [-319380. -633740. -276510. ... -456940. -496990. -422960.]\n",
      "gradient 5.339520187093092e+27\n",
      "gradient magnitude 73072020548860.52\n",
      "\n",
      " predictions [421002.96577892 792804.3785608  603427.43848423 ... 534683.2867589\n",
      " 526304.56305387 323122.22062344]\n",
      "errors [199102.96577892 254804.3785608  423427.43848423 ... 174683.2867589\n",
      " 126304.56305387  -1877.77937656]\n",
      "gradient 5.140749489327198e+26\n",
      "gradient magnitude 22673220965110.355\n",
      "\n",
      " predictions [259518.53331584 519010.81199792 380998.2189088  ... 338458.60344862\n",
      " 333152.72488025 192305.75558163]\n",
      "errors [  37618.53331584  -18989.18800208  200998.2189088  ...  -21541.39655138\n",
      "  -66847.27511975 -132694.24441837]\n",
      "gradient 4.985482013056992e+25\n",
      "gradient magnitude 7060794582096.97\n",
      "\n",
      " predictions [308947.89676147 605683.9180731  444775.73350445 ... 399005.83731368\n",
      " 393224.89063921 232670.58074683]\n",
      "errors [ 87047.89676147  67683.9180731  264775.73350445 ...  39005.83731368\n",
      "  -6775.10936079 -92329.41925317]\n",
      "gradient 5.178730359570032e+24\n",
      "gradient magnitude 2275682394265.516\n",
      "\n",
      " predictions [292976.07471613 580513.52215816 419907.35778441 ... 379920.45263373\n",
      " 374753.51300031 219946.9898073 ]\n",
      "errors [  71076.07471613   42513.52215816  239907.35778441 ...   19920.45263373\n",
      "  -25246.48699969 -105053.0101927 ]\n",
      "gradient 8.630114685288e+23\n",
      "gradient magnitude 928984105638.4119\n",
      "\n",
      " predictions [297304.20852359 589991.74762577 422653.98204139 ... 385541.08518485\n",
      " 380639.29427419 223694.07819052]\n",
      "errors [  75404.20852359   51991.74762577  242653.98204139 ...   25541.08518485\n",
      "  -19360.70572581 -101305.92180948]\n",
      "gradient 4.307394363438706e+23\n",
      "gradient magnitude 656307425178.072\n",
      "\n",
      " predictions [295350.9780301  588683.74349714 416953.40939044 ... 383506.02116493\n",
      " 378966.77101404 222337.36850529]\n",
      "errors [  73450.9780301    50683.74349714  236953.40939044 ...   23506.02116493\n",
      "  -21033.22898596 -102662.63149471]\n",
      "gradient 3.728511078798657e+23\n",
      "gradient magnitude 610615351821.3129\n",
      "\n",
      " predictions [295360.46021516 590682.46752576 413987.85011566 ... 383852.36122424\n",
      " 379634.89188271 222568.26165735]\n",
      "errors [  73460.46021516   52682.46752576  233987.85011566 ...   23852.36122424\n",
      "  -20365.10811729 -102431.73834265]\n",
      "gradient 3.517424218578938e+23\n",
      "gradient magnitude 593078765306.8468\n",
      "\n",
      " predictions [294775.3385645  591617.78404005 410286.31428274 ... 383466.84646038\n",
      " 379573.49740894 222311.25156511]\n",
      "errors [  72875.3385645    53617.78404005  230286.31428274 ...   23466.84646038\n",
      "  -20426.50259106 -102688.74843489]\n",
      "gradient 3.3490054199156494e+23\n",
      "gradient magnitude 578705920128.3196\n",
      "\n",
      " predictions [294388.45149083 592846.02566187 406922.69506887 ... 383314.92102722\n",
      " 379734.9085226  222209.9677098 ]\n",
      "errors [  72488.45149083   54846.02566187  226922.69506887 ...   23314.92102722\n",
      "  -20265.0914774  -102790.0322902 ]\n",
      "gradient 3.1916361701548006e+23\n",
      "gradient magnitude 564945676163.1865\n",
      "\n",
      " predictions [293953.56548726 593947.38945394 403561.31358388 ... 383096.98741741\n",
      " 379823.83664142 222064.67840561]\n",
      "errors [  72053.56548726   55947.38945394  223561.31358388 ...   23096.98741741\n",
      "  -20176.16335858 -102935.32159439]\n",
      "gradient 3.041949171075395e+23\n",
      "gradient magnitude 551538681424.5575\n",
      "\n",
      " predictions [293546.7320767  595052.93570203 400303.74378086 ... 382905.81558582\n",
      " 379931.94824956 221937.23029337]\n",
      "errors [  71646.7320767    57052.93570203  220303.74378086 ...   22905.81558582\n",
      "  -20068.05175044 -103062.76970663]\n",
      "gradient 2.8993101170432125e+23\n",
      "gradient magnitude 538452422879.0518\n",
      "\n",
      " predictions [293144.0508495  596122.84936036 397116.00207487 ... 382712.48290957\n",
      " 380030.89042764 221808.34162301]\n",
      "errors [  71244.0508495    58122.84936036  217116.00207487 ...   22712.48290957\n",
      "  -19969.10957236 -103191.65837699]\n",
      "gradient 2.7633621655443425e+23\n",
      "gradient magnitude 525676912708.2092\n",
      "\n",
      " predictions [292752.62977429 597170.29406504 394006.20806852 ... 382525.81432449\n",
      " 380129.53346873 221683.89568536]\n",
      "errors [  70852.62977429   59170.29406504  214006.20806852 ...   22525.81432449\n",
      "  -19870.46653127 -103316.10431464]\n",
      "gradient 2.6337890366403754e+23\n",
      "gradient magnitude 513204543689.97705\n",
      "\n",
      " predictions [292369.96653508 598191.98232144 390969.48012645 ... 382342.93048673\n",
      " 380225.20074828 221561.97291762]\n",
      "errors [  70469.96653508   60191.98232144  210969.48012645 ...   22342.93048673\n",
      "  -19774.79925172 -103438.02708238]\n",
      "gradient 2.510291573093381e+23\n",
      "gradient magnitude 501028100319.072\n",
      "\n",
      " predictions [291996.54659354 599189.71022714 388005.02513274 ... 382164.58561146\n",
      " 380318.79524995 221443.07612981]\n",
      "errors [  70096.54659354   61189.71022714  208005.02513274 ...   22164.58561146\n",
      "  -19681.20475005 -103556.92387019]\n",
      "gradient 2.392584865574763e+23\n",
      "gradient magnitude 489140559100.8338\n",
      "\n",
      " predictions [291631.93562206 600163.67873673 385110.83662264 ... 381990.41022782\n",
      " 380410.10798318 221326.95900791]\n",
      "errors [  69731.93562206   62163.67873673  205110.83662264 ...   21990.41022782\n",
      "  -19589.89201682 -103673.04099209]\n",
      "gradient 2.2803973852708e+23\n",
      "gradient magnitude 477535065232.9942\n",
      "\n",
      " predictions [291275.99130624 601114.56556712 382285.33790459 ... 381820.38660199\n",
      " 380499.27315954 221213.60972925]\n",
      "errors [  69375.99130624   63114.56556712  202285.33790459 ...   21820.38660199\n",
      "  -19500.72684046 -103786.39027075]\n",
      "gradient 2.1734703373011478e+23\n",
      "gradient magnitude 466204926754.44214\n",
      "\n",
      " predictions [290928.487334   602042.88300471 379526.87116361 ... 381654.39104549\n",
      " 380586.31689706 221102.94583473]\n",
      "errors [  69028.487334     64042.88300471  199526.87116361 ...   21654.39104549\n",
      "  -19413.68310294 -103897.05416527]\n",
      "gradient 2.0715570617845837e+23\n",
      "gradient magnitude 455143610499.4317\n",
      "\n",
      " predictions [290589.22986271 602949.17750369 376833.85468238 ... 381492.33579776\n",
      " 380671.29723259 220994.90881721]\n",
      "errors [  68689.22986271   64949.17750369  196833.85468238 ...   21492.33579776\n",
      "  -19328.70276741 -104005.09118279]\n",
      "gradient 1.9744224646556435e+23\n",
      "gradient magnitude 444344738312.0052\n",
      "\n",
      " predictions [290258.02124364 603833.96818559 374204.73286926 ... 381334.12494734\n",
      " 380754.26073386 220889.4347356 ]\n",
      "errors [  68358.02124364   65833.96818559  194204.73286926 ...   21334.12494734\n",
      "  -19245.73926614 -104110.5652644 ]\n",
      "gradient 1.881842475330437e+23\n",
      "gradient magnitude 433802083366.4169\n",
      "\n",
      " predictions [289934.67112448 604697.76631127 371637.99057765 ... 381179.66803229\n",
      " 380835.25599391 220786.46328185]\n",
      "errors [  68034.67112448   66697.76631127  191637.99057765 ...   21179.66803229\n",
      "  -19164.74400609 -104213.53671815]\n",
      "gradient 1.793603529817751e+23\n",
      "gradient magnitude 423509566576.45306\n",
      "\n",
      " predictions [289618.99286072 605541.06962915 369132.14751287 ... 381028.87575258\n",
      " 380914.32948197 220685.93492249]\n",
      "errors [  67718.99286072   67541.06962915  189132.14751287 ...   21028.87575258\n",
      "  -19085.67051803 -104314.06507751]\n",
      "gradient 1.709502078068386e+23\n",
      "gradient magnitude 413461253090.10345\n",
      "\n",
      " predictions [289310.80448702 606364.3645066  366685.75884235 ... 380881.66123228\n",
      " 380991.52686586 220587.79174014]\n",
      "errors [  67410.80448702   68364.3645066   186685.75884235 ...   20881.66123228\n",
      "  -19008.47313414 -104412.20825986]\n",
      "gradient 1.62934411442484e+23\n",
      "gradient magnitude 403651348867.4155\n",
      "\n",
      " predictions [289009.92827761 607168.12563832 364297.41390854 ... 380737.93956197\n",
      " 381066.89263647 220491.97712843]\n",
      "errors [  67109.92827761   69168.12563832  184297.41390854 ...   20737.93956197\n",
      "  -18933.10736353 -104508.02287157]\n",
      "gradient 1.5529447300880662e+23\n",
      "gradient magnitude 394074197339.545\n",
      "\n",
      " predictions [288716.19074763 607952.81649753 361965.73555627 ... 380597.62787622\n",
      " 381140.47025807 220398.43584363]\n",
      "errors [  66816.19074763   69952.81649753  181965.73555627 ...   20597.62787622\n",
      "  -18859.52974193 -104601.56415637]\n",
      "gradient 1.4801276865689249e+23\n",
      "gradient magnitude 384724276147.07715\n",
      "\n",
      " predictions [288429.42252088 608718.88954812 359689.37929505 ... 380460.64526657\n",
      " 381212.30215467 220307.1139467 ]\n",
      "errors [  66529.42252088   70718.88954812  179689.37929505 ...   20460.64526657\n",
      "  -18787.69784533 -104692.8860533 ]\n",
      "gradient 1.4107250091403253e+23\n",
      "gradient magnitude 375596193955.7329\n",
      "\n",
      " predictions [288149.45824215 609466.78652268 357467.03253729 ... 380326.91274705\n",
      " 381282.42974653 220217.9587802 ]\n",
      "errors [  66249.45824215   71466.78652268  177467.03253729 ...   20326.91274705\n",
      "  -18717.57025347 -104782.0412198 ]\n",
      "gradient 1.3445765993522822e+23\n",
      "gradient magnitude 366684687347.6287\n",
      "\n",
      " predictions [287876.1364788  610196.93867189 355297.41383727 ... 380196.35320483\n",
      " 381350.89347028 220130.91893553]\n",
      "errors [  65976.1364788    72196.93867189  175297.41383727 ...   20196.35320483\n",
      "  -18649.10652972 -104869.08106447]\n",
      "gradient 1.2815298657159785e+23\n",
      "gradient magnitude 357984617786.2924\n",
      "\n",
      " predictions [287609.29962857 610909.76701483 353179.27215354 ... 380068.89135699\n",
      " 381417.73280341 220045.94422397]\n",
      "errors [  65709.29962857   72909.76701483  173179.27215354 ...   20068.89135699\n",
      "  -18582.26719659 -104954.05577603]\n",
      "gradient 1.2214393717049608e+23\n",
      "gradient magnitude 349490968653.6922\n",
      "\n",
      " predictions [287348.7938285  611605.68258122 351111.38612714 ... 379944.4537067\n",
      " 381482.98628668 219962.98564754]\n",
      "errors [  65448.7938285    73605.68258122  171111.38612714 ...   19944.4537067\n",
      "  -18517.01371332 -105037.01435246]\n",
      "gradient 1.1641665002614014e+23\n",
      "gradient magnitude 341198842357.5616\n",
      "\n",
      " predictions [287094.4688662  612285.08664863 349092.56337745 ... 379822.96850095\n",
      " 381546.69154644 219881.99537084]\n",
      "errors [  65194.4688662    74285.08664863  169092.56337745 ...   19822.96850095\n",
      "  -18453.30845356 -105118.00462916]\n",
      "gradient 1.1095791340335757e+23\n",
      "gradient magnitude 333103457507.36\n",
      "\n",
      " predictions [286846.17809329 612948.37097376 347121.63981463 ... 379704.36568916\n",
      " 381608.88531628 219802.92669339]\n",
      "errors [  64946.17809329   74948.37097376  167121.63981463 ...   19704.36568916\n",
      "  -18391.11468372 -105197.07330661]\n",
      "gradient 1.0575513506068753e+23\n",
      "gradient magnitude 325200146157.2358\n",
      "\n",
      " predictions [286603.77834081 613595.91801841 345197.47896837 ... 379588.5768828\n",
      " 381669.60345828 219725.73402276]\n",
      "errors [  64703.77834081   75595.91801841  165197.47896837 ...   19588.5768828\n",
      "  -18330.39654172 -105274.26597724]\n",
      "gradient 1.0079631320253411e+23\n",
      "gradient magnitude 317484351114.40393\n",
      "\n",
      " predictions [286367.12983665 614228.10116995 343318.9713326  ... 379475.53531595\n",
      " 381728.88098362 219650.37284827]\n",
      "errors [  64467.12983665   76228.10116995  163318.9713326  ...   19475.53531595\n",
      "  -18271.11901638 -105349.62715173]\n",
      "gradient 9.607000879336335e+22\n",
      "gradient magnitude 309951623311.38605\n",
      "\n",
      " predictions [286136.096125   614845.28495667 341485.03372573 ... 379365.17580679\n",
      " 381786.75207278 219576.79971528]\n",
      "errors [  64236.096125     76845.28495667  161485.03372573 ...   19365.17580679\n",
      "  -18213.24792722 -105423.20028472]\n",
      "gradient 9.156531917007755e+22\n",
      "gradient magnitude 302597619240.5974\n",
      "\n",
      " predictions [285910.54398762 615447.82525793 339694.60866603 ... 379257.43472002\n",
      " 381843.25009531 219504.97220021]\n",
      "errors [  64010.54398762   77447.82525793  159694.60866603 ...   19257.43472002\n",
      "  -18156.74990469 -105495.02779979]\n",
      "gradient 8.727185289169595e+22\n",
      "gradient magnitude 295418098449.80035\n",
      "\n",
      " predictions [285690.34336706 616036.06950942 337946.66376192 ... 379152.24993017\n",
      " 381898.40762897 219434.84888599]\n",
      "errors [  63790.34336706   78036.06950942  157946.66376192 ...   19152.24993017\n",
      "  -18101.59237103 -105565.15111401]\n",
      "gradient 8.317970576832723e+22\n",
      "gradient magnitude 288408921096.98553\n",
      "\n",
      " predictions [285475.36729165 616610.35690345 336240.19111664 ... 379049.56078579\n",
      " 381952.25647861 219366.38933825]\n",
      "errors [  63575.36729165   78610.35690345  156240.19111664 ...   19049.56078579\n",
      "  -18047.74352139 -105633.61066175]\n",
      "gradient 7.927943801413204e+22\n",
      "gradient magnitude 281566045563.26044\n",
      "\n",
      " predictions [285265.49180227 617171.01858455 334574.20674708 ... 378949.30807446\n",
      " 382004.82769445 219299.55408199]\n",
      "errors [  63365.49180227   79171.01858455  154574.20674708 ...   18949.30807446\n",
      "  -17995.17230555 -105700.44591801]\n",
      "gradient 7.556205247157839e+22\n",
      "gradient magnitude 274885526122.38135\n",
      "\n",
      " predictions [285060.59588091 617718.37784043 332947.75001642 ... 378851.43398863\n",
      " 382056.15159001 219234.30457877]\n",
      "errors [  63160.59588091   79718.37784043  152947.75001642 ...   18851.43398863\n",
      "  -17943.84840999 -105765.69542123]\n",
      "gradient 7.201897385675125e+22\n",
      "gradient magnitude 268363510665.57327\n",
      "\n",
      " predictions [284860.56138087 618252.75028837 331359.88308017 ... 378755.88209234\n",
      " 382106.25775958 219170.60320455]\n",
      "errors [  62960.56138087   80252.75028837  151359.88308017 ...   18755.88209234\n",
      "  -17893.74224042 -105829.39679545]\n",
      "gradient 6.8642028977845875e+22\n",
      "gradient magnitude 261996238480.3375\n",
      "\n",
      " predictions [284665.27295862 618774.44405723 329809.69034545 ... 378662.59728863\n",
      " 382155.17509529 219108.41322796]\n",
      "errors [  62765.27295862   80774.44405723  149809.69034545 ...   18662.59728863\n",
      "  -17844.82490471 -105891.58677204]\n",
      "gradient 6.5423427881205395e+22\n",
      "gradient magnitude 255780038081.95312\n",
      "\n",
      " predictions [284474.61800732 619283.7599651  328296.27794297 ... 378571.52578779\n",
      " 382202.93180376 219047.69878914]\n",
      "errors [  62574.61800732   81283.7599651   148296.27794297 ...   18571.52578779\n",
      "  -17797.06819624 -105952.30121086]\n",
      "gradient 6.235574588141701e+22\n",
      "gradient magnitude 249711325096.4341\n",
      "\n",
      " predictions [284288.48659186 619780.99169277 326818.77321169 ... 378482.61507635\n",
      " 382249.55542238 218988.42487904]\n",
      "errors [  62388.48659186   81780.99169277  146818.77321169 ...   18482.61507635\n",
      "  -17750.44457762 -106011.57512096]\n",
      "gradient 5.9431906434009855e+22\n",
      "gradient magnitude 243786600193.7142\n",
      "\n",
      " predictions [284106.77138548 620266.4259531  325376.32419554 ... 378395.81388676\n",
      " 382295.07283517 218930.55731924]\n",
      "errors [  62206.77138548   82266.4259531   145376.32419554 ...   18395.81388676\n",
      "  -17704.92716483 -106069.44268076]\n",
      "gradient 5.664516481124597e+22\n",
      "gradient magnitude 238002447069.86935\n",
      "\n",
      " predictions [283929.3676079  620740.34265629 323968.09915225 ... 378311.07216789\n",
      " 382339.5102883  218874.06274227]\n",
      "errors [  62029.3676079    82740.34265629  143968.09915225 ...   18311.07216789\n",
      "  -17660.4897117  -106125.93725773]\n",
      "gradient 5.398909254334749e+22\n",
      "gradient magnitude 232355530477.21393\n",
      "\n",
      " predictions [283756.17296489 621203.0150713  322593.28607369 ... 378228.34105612\n",
      " 382382.89340522 218818.90857233]\n",
      "errors [  61856.17296489   83203.0150713   142593.28607369 ...   18228.34105612\n",
      "  -17617.10659478 -106181.09142767]\n",
      "gradient 5.145756258927122e+22\n",
      "gradient magnitude 226842594301.1392\n",
      "\n",
      " predictions [283587.08758927 621654.70998347 321251.09221768 ... 378147.57284721\n",
      " 382425.24720142 218765.06300651]\n",
      "errors [  61687.08758927   83654.70998347  141251.09221768 ...   18147.57284721\n",
      "  -17574.75279858 -106234.93699349]\n",
      "gradient 4.904473520281714e+22\n",
      "gradient magnitude 221460459682.57437\n",
      "\n",
      " predictions [283422.01398335 622095.68784826 319940.74365088 ... 378068.72096872\n",
      " 382466.59609886 218712.4949965 ]\n",
      "errors [  61522.01398335   84095.68784826  139940.74365088 ...   18068.72096872\n",
      "  -17533.40390114 -106287.5050035 ]\n",
      "gradient 4.674504446147395e+22\n",
      "gradient magnitude 216206023185.00275\n",
      "\n",
      " predictions [283260.85696268 622526.20294152 318661.48480253 ... 377991.73995325\n",
      " 382506.96394007 218661.17423062]\n",
      "errors [  61360.85696268   84526.20294152  138661.48480253 ...   17991.73995325\n",
      "  -17493.03605993 -106338.82576938]\n",
      "gradient 4.455318542691942e+22\n",
      "gradient magnitude 211076255004.96124\n",
      "\n",
      " predictions [283103.52360121 622946.50350608 317412.57802875 ... 377916.58541213\n",
      " 382546.37400189 218611.07111639]\n",
      "errors [  61203.52360121   84946.50350608  137412.57802875 ...   17916.58541213\n",
      "  -17453.62599811 -106388.92888361]\n",
      "gradient 4.2464101907562504e+22\n",
      "gradient magnitude 206068197225.00244\n",
      "\n",
      " predictions [282949.92317766 623356.83189486 316193.30318722 ... 377843.2140099\n",
      " 382584.84900887 218562.15676345]\n",
      "errors [  61049.92317766   85356.83189486  136193.30318722 ...   17843.2140099\n",
      "  -17415.15099113 -106437.84323655]\n",
      "gradient 4.0472974794893755e+22\n",
      "gradient magnitude 201178962108.10352\n",
      "\n",
      " predictions [282799.96712324 623757.42471066 315002.95722194 ... 377771.58343927\n",
      " 382622.41114639 218514.4029669 ]\n",
      "errors [  60899.96712324   85757.42471066  135002.95722194 ...   17771.58343927\n",
      "  -17377.58885361 -106485.5970331 ]\n",
      "gradient 3.857521094674264e+22\n",
      "gradient magnitude 196405730432.54782\n",
      "\n",
      " predictions [282653.56897058 624148.51294258 313840.85375781 ... 377701.65239676\n",
      " 382659.08207348 218467.78219102]\n",
      "errors [  60753.56897058   86148.51294258  133840.85375781 ...   17701.65239676\n",
      "  -17340.91792652 -106532.21780898]\n",
      "gradient 3.6766432591792442e+22\n",
      "gradient magnitude 191745749866.3072\n",
      "\n",
      " predictions [282510.64430383 624530.32209919 312706.32270489 ... 377633.38055887\n",
      " 382694.88293524 218422.26755343]\n",
      "errors [  60610.64430383   86530.32209919  132706.32270489 ...   17633.38055887\n",
      "  -17305.11706476 -106577.73244657]\n",
      "gradient 3.5042467230915857e+22\n",
      "gradient magnitude 187196333379.99936\n",
      "\n",
      " predictions [282371.11071005 624903.07233857 311598.70987197 ... 377566.7285588\n",
      " 382729.83437512 218377.83280958]\n",
      "errors [  60471.11071005   86903.07233857  131598.70987197 ...   17566.7285588\n",
      "  -17270.16562488 -106622.16719042]\n",
      "gradient 3.3399338012032568e+22\n",
      "gradient magnitude 182754857697.49753\n",
      "\n",
      " predictions [282234.88773163 625266.97859528 310517.37658939 ... 377501.65796381\n",
      " 382763.95654675 218334.45233758]\n",
      "errors [  60334.88773163   87266.97859528  130517.37658939 ...   17501.65796381\n",
      "  -17236.04345325 -106665.54766242]\n",
      "gradient 3.1833254556286677e+22\n",
      "gradient magnitude 178418761783.30203\n",
      "\n",
      " predictions [282101.89681992 625622.25070429 309461.69934075 ... 377438.13125301\n",
      " 382797.26912558 218292.10112346]\n",
      "errors [  60201.89681992   87622.25070429  129461.69934075 ...   17438.13125301\n",
      "  -17202.73087442 -106707.89887654]\n",
      "gradient 3.034060421438109e+22\n",
      "gradient magnitude 174185545365.79977\n",
      "\n",
      " predictions [281972.06128997 625969.09352192 308431.06940339 ... 377376.11179575\n",
      " 382829.79132026 218250.75474675]\n",
      "errors [  60072.06128997   87969.09352192  128431.06940339 ...   17376.11179575\n",
      "  -17170.20867974 -106749.24525325]\n",
      "gradient 2.8917943732900226e+22\n",
      "gradient magnitude 170052767495.56363\n",
      "\n",
      " predictions [281845.30627624 626307.70704406 307424.89249737 ... 377315.56383048\n",
      " 382861.54188368 218210.38936638]\n",
      "errors [  59945.30627624   88307.70704406  127424.89249737 ...   17315.56383048\n",
      "  -17138.45811632 -106789.61063362]\n",
      "gradient 2.756199131139496e+22\n",
      "gradient magnitude 166018045137.85532\n",
      "\n",
      " predictions [281721.55868949 626638.28652139 306442.58844283 ... 377256.45244415\n",
      " 382892.53912379 218170.98170693]\n",
      "errors [  59821.55868949   88638.28652139  126442.58844283 ...   17256.45244415\n",
      "  -17107.46087621 -106829.01829307]\n",
      "gradient 2.6269619031909127e+22\n",
      "gradient magnitude 162079051798.52554\n",
      "\n",
      " predictions [281600.74717462 626961.02257206 305483.59082541 ... 377198.74355206\n",
      " 382922.80091416 218132.50904524]\n",
      "errors [  59700.74717462   88961.02257206  125483.59082541 ...   17198.74355206\n",
      "  -17077.19908584 -106867.49095476]\n",
      "gradient 2.503784564348192e+22\n",
      "gradient magnitude 158233516182.514\n",
      "\n",
      " predictions [281482.80206952 627276.10129153 304547.34666969 ... 377142.40387822\n",
      " 382952.34470429 218094.94919728]\n",
      "errors [  59582.80206952   89276.10129153  124547.34666969 ...   17142.40387822\n",
      "  -17047.65529571 -106905.05080272]\n",
      "gradient 2.386382968498366e+22\n",
      "gradient magnitude 154479220884.18124\n",
      "\n",
      " predictions [281367.6553649  627583.7043599  303633.31612029 ... 377087.40093615\n",
      " 382981.18752967 218058.28050535]\n",
      "errors [  59467.6553649    89583.7043599   123633.31612029 ...   17087.40093615\n",
      "  -17018.81247033 -106941.71949465]\n",
      "gradient 2.274486293041787e+22\n",
      "gradient magnitude 150814001108.70963\n",
      "\n",
      " predictions [281255.24066508 627884.00914669 302740.97213059 ... 377033.70301016\n",
      " 383009.34602159 218022.48182562]\n",
      "errors [  59355.24066508   89884.00914669  122740.97213059 ...   17033.70301016\n",
      "  -16990.65397841 -106977.51817438]\n",
      "gradient 2.167836414157264e+22\n",
      "gradient magnitude 147235743423.8461\n",
      "\n",
      " predictions [281145.4931497  628177.18881309 301869.80015887 ... 376981.27913704\n",
      " 383036.83641676 217987.53251592]\n",
      "errors [  59245.4931497    90177.18881309  121869.80015887 ...   16981.27913704\n",
      "  -16963.16358324 -107012.46748408]\n",
      "gradient 2.0661873113606706e+22\n",
      "gradient magnitude 143742384541.2574\n",
      "\n",
      " predictions [281038.34953637 628463.41241181 301019.29787154 ... 376930.09908824\n",
      " 383063.67456662 217953.41242385]\n",
      "errors [  59138.34953637   90463.41241181  121019.29787154 ...   16930.09908824\n",
      "  -16936.32543338 -107046.58757615]\n",
      "gradient 1.969304499983639e+22\n",
      "gradient magnitude 140331910126.8004\n",
      "\n",
      " predictions [280933.74804412 628742.84498459 300188.97485355 ... 376880.13335242\n",
      " 383089.87594656 217920.10187515]\n",
      "errors [  59033.74804412   90742.84498459  120188.97485355 ...   16880.13335242\n",
      "  -16910.12405344 -107079.89812485]\n",
      "gradient 1.876964490263128e+22\n",
      "gradient magnitude 137002353639.02066\n",
      "\n",
      " predictions [280831.62835783 629015.64765732 299378.35232557 ... 376831.35311843\n",
      " 383115.45566476 217887.58166235]\n",
      "errors [  58931.62835783   91015.64765732  119378.35232557 ...   16831.35311843\n",
      "  -16884.54433524 -107112.41833765]\n",
      "gradient 1.7889542717941286e+22\n",
      "gradient magnitude 133751795195.20958\n",
      "\n",
      " predictions [280731.93159346 629281.97773296 298586.96286794 ... 376783.73025869\n",
      " 383140.42847095 217855.83303371]\n",
      "errors [  58831.93159346   91281.97773296  118586.96286794 ...   16783.73025869\n",
      "  -16859.57152905 -107144.16696629]\n",
      "gradient 1.7050708221561594e+22\n",
      "gradient magnitude 130578360464.36482\n",
      "\n",
      " predictions [280634.60026402 629541.98878229 297814.35015113 ... 376737.23731301\n",
      " 383164.8087649  217824.83768239]\n",
      "errors [  58734.60026402   91541.98878229  117814.35015113 ...   16737.23731301\n",
      "  -16835.1912351  -107175.16231761]\n",
      "gradient 1.6251206385800762e+22\n",
      "gradient magnitude 127480219586.41568\n",
      "\n",
      " predictions [280539.57824651 629795.83073237 297060.06867263 ... 376691.84747271\n",
      " 383188.61060474 217794.57773591]\n",
      "errors [  58639.57824651   91795.83073237  117060.06867263 ...   16691.84747271\n",
      "  -16811.38939526 -107205.42226409]\n",
      "gradient 1.5489192915749797e+22\n",
      "gradient magnitude 124455586117.0956\n",
      "\n",
      " predictions [280446.81074951 630043.64995311 296323.68350004 ... 376647.53456518\n",
      " 383211.84771505 217765.03574584]\n",
      "errors [  58546.81074951   92043.64995311  116323.68350004 ...   16647.53456518\n",
      "  -16788.15228495 -107234.96425416]\n",
      "gradient 1.47629099948524e+22\n",
      "gradient magnitude 121502715997.84262\n",
      "\n",
      " predictions [280356.2442816  630285.58934154 295604.77002031 ... 376604.2730388\n",
      " 383234.53349475 217736.19467772]\n",
      "errors [  58456.2442816    92285.58934154  115604.77002031 ...   16604.2730388\n",
      "  -16765.46650525 -107263.80532228]\n",
      "gradient 1.4070682229965196e+22\n",
      "gradient magnitude 118619906550.14526\n",
      "\n",
      " predictions [280267.82662051 630521.78840433 294902.91369486 ... 376562.03794819\n",
      " 383256.68102489 217708.03790129]\n",
      "errors [  58367.82662051   92521.78840433  114902.91369486 ...   16562.03794819\n",
      "  -16743.31897511 -107291.96209871]\n",
      "gradient 1.3410912786552554e+22\n",
      "gradient magnitude 115805495493.74828\n",
      "\n",
      " predictions [280181.50678301 630752.38333813 294217.70982061 ... 376520.80493982\n",
      " 383278.30307613 217680.54918083]\n",
      "errors [  58281.50678301   92752.38333813  114217.70982061 ...   16520.80493982\n",
      "  -16721.69692387 -107319.45081917]\n",
      "gradient 1.2782079705099903e+22\n",
      "gradient magnitude 113057859988.149\n",
      "\n",
      " predictions [280097.23499553 630977.50710817 293548.76329657 ... 376480.550238\n",
      " 383299.41211614 217653.71266585]\n",
      "errors [  58197.23499553   92977.50710817  113548.76329657 ...   16480.550238\n",
      "  -16700.58788386 -107346.28733415]\n",
      "gradient 1.2182732390250138e+22\n",
      "gradient magnitude 110375415696.83957\n",
      "\n",
      " predictions [280014.96266541 631197.28952491 292895.68839601 ... 376441.25063114\n",
      " 383320.02031678 217627.51288193]\n",
      "errors [  58114.96266541   93197.28952491  112895.68839601 ...   16441.25063114\n",
      "  -16679.97968322 -107372.48711807]\n",
      "gradient 1.1611488264563123e+22\n",
      "gradient magnitude 107756615873.75098\n",
      "\n",
      " predictions [279934.64235294 631411.85731888 292258.10854411 ... 376402.88345837\n",
      " 383340.13956111 217601.93472179]\n",
      "errors [  58034.64235294   93411.85731888  112258.10854411 ...   16402.88345837\n",
      "  -16659.86043889 -107398.06527821]\n",
      "gradient 1.1067029579178233e+22\n",
      "gradient magnitude 105199950471.36778\n",
      "\n",
      " predictions [279856.22774397 631621.33421375 291635.65610074 ... 376365.42659648\n",
      " 383359.78145025 217576.96343661]\n",
      "errors [  57956.22774397   93621.33421375  111635.65610074 ...   16365.42659648\n",
      "  -16640.21854975 -107423.03656339]\n",
      "gradient 1.0548100374025506e+22\n",
      "gradient magnitude 102703945270.01144\n",
      "\n",
      " predictions [279779.67362321 631825.84099772 291027.97214853 ... 376328.85844717\n",
      " 383378.95731008 217552.5846275 ]\n",
      "errors [  57879.67362321   93825.84099772  111027.97214853 ...   16328.85844717\n",
      "  -16621.04268992 -107447.4153725 ]\n",
      "gradient 1.0053503580569749e+22\n",
      "gradient magnitude 100267161027.77493\n",
      "\n",
      " predictions [279704.93584818 632025.4955931  290434.70628587 ... 376293.15792457\n",
      " 383397.67819773 217528.78423718]\n",
      "errors [  57804.93584818   94025.4955931   110434.70628587 ...   16293.15792457\n",
      "  -16602.32180227 -107471.21576282]\n",
      "gradient 9.582098260407248e+21\n",
      "gradient magnitude 97888192650.63202\n",
      "\n",
      " predictions [279631.9713237  632220.41312437 289855.51642491 ... 376258.30444313\n",
      " 383415.95490803 217505.54854193]\n",
      "errors [  57731.9713237    94220.41312437  109855.51642491 ...   16258.30444313\n",
      "  -16584.04509197 -107494.45145807]\n",
      "gradient 9.132796973342922e+21\n",
      "gradient magnitude 95565668382.23297\n",
      "\n",
      " predictions [279560.73797712 632410.7059845  289290.06859425 ... 376224.27790569\n",
      " 383433.79797965 217482.86414362]\n",
      "errors [  57660.73797712   94410.7059845   109290.06859425 ...   16224.27790569\n",
      "  -16566.20202035 -107517.13585638]\n",
      "gradient 8.704563268878212e+21\n",
      "gradient magnitude 93298249012.92741\n",
      "\n",
      " predictions [279491.194734   632596.48389981 288738.03674642 ... 376191.05869193\n",
      " 383451.21770123 217460.71796201]\n",
      "errors [  57591.194734     94596.48389981  108738.03674642 ...   16191.05869193\n",
      "  -16548.78229877 -107539.28203799]\n",
      "gradient 8.296409295321016e+21\n",
      "gradient magnitude 91084627107.54771\n",
      "\n",
      " predictions [279423.30149443 632777.85399323 288199.10256982 ... 376158.62764705\n",
      " 383468.2241173  217439.09722723]\n",
      "errors [  57523.30149443   94777.85399323  108199.10256982 ...   16158.62764705\n",
      "  -16531.7758827  -107560.90277277]\n",
      "gradient 7.907393521005158e+21\n",
      "gradient magnitude 88923526251.5222\n",
      "\n",
      " predictions [279357.01910995 632954.92084604 287672.95530523 ... 376126.96607072\n",
      " 383484.82703407 217417.98947235]\n",
      "errors [  57457.01910995   94954.92084604  107672.95530523 ...   16126.96607072\n",
      "  -16515.17296593 -107582.01052765]\n",
      "gradient 7.536618562359975e+21\n",
      "gradient magnitude 86813700314.8695\n",
      "\n",
      " predictions [279292.30936093 633127.78655821 287159.29156658 ... 376096.05570631\n",
      " 383501.03602508 217397.38252628]\n",
      "errors [  57392.30936093   95127.78655821  107159.29156658 ...   16096.05570631\n",
      "  -16498.96397492 -107602.61747372]\n",
      "gradient 7.183229113821124e+21\n",
      "gradient magnitude 84753932733.65622\n",
      "\n",
      " predictions [279229.13493455 633296.55080724 286657.81516604 ... 376065.87873034\n",
      " 383516.86043674 217377.26450666]\n",
      "errors [  57329.13493455   95296.55080724  106657.81516604 ...   16065.87873034\n",
      "  -16483.13956326 -107622.73549334]\n",
      "gradient 6.846409974807871e+21\n",
      "gradient magnitude 82743035808.507\n",
      "\n",
      " predictions [279167.4594033  633461.31090566 286168.23694321 ... 376036.41774221\n",
      " 383532.30939368 217357.62381309]\n",
      "errors [  57267.4594033    95461.31090566  106168.23694321 ...   16036.41774221\n",
      "  -16467.69060632 -107642.37618691]\n",
      "gradient 6.525384169213693e+21\n",
      "gradient magnitude 80779850019.75241\n",
      "\n",
      " predictions [279107.24720395 633622.16185714 285690.27459841 ... 376007.65575419\n",
      " 383547.39180405 217338.44912038]\n",
      "errors [  57207.24720395   95622.16185714  105690.27459841 ...   16007.65575419\n",
      "  -16452.60819595 -107661.55087962]\n",
      "gradient 6.219411153074297e+21\n",
      "gradient magnitude 78863243358.8316\n",
      "\n",
      " predictions [279048.46361704 633779.19641125 285223.65252986 ... 375979.57618159\n",
      " 383562.11636466 217319.72937207]\n",
      "errors [  57148.46361704   95779.19641125  105223.65252986 ...   15979.57618159\n",
      "  -16437.88363534 -107680.27062793]\n",
      "gradient 5.927785106276639e+21\n",
      "gradient magnitude 76992110675.55324\n",
      "\n",
      " predictions [278991.07474691 633932.50511698 284768.10167481 ... 375952.16283322\n",
      " 383576.49156594 217301.453774  ]\n",
      "errors [  57091.07474691   95932.50511698  104768.10167481 ...   15952.16283322\n",
      "  -16423.50843406 -107698.546226  ]\n",
      "gradient 5.649833304369784e+21\n",
      "gradient magnitude 75165373040.84763\n",
      "\n",
      " predictions [278935.04750207 634082.17637489 284323.35935438 ... 375925.39990206\n",
      " 383590.5256969  217283.61178814]\n",
      "errors [  57035.04750207   96082.17637489  104323.35935438 ...   15925.39990206\n",
      "  -16409.4743031  -107716.38821186]\n",
      "gradient 5.384914566720617e+21\n",
      "gradient magnitude 73381977124.63611\n",
      "\n",
      " predictions [278880.34957621 634228.29648817 283889.16912205 ... 375899.2719561\n",
      " 383604.22684988 217266.19312647]\n",
      "errors [  56980.34957621   96228.29648817  103889.16912205 ...   15899.2719561\n",
      "  -16395.77315012 -107733.80687353]\n",
      "gradient 5.132417777434934e+21\n",
      "gradient magnitude 71640894588.46068\n",
      "\n",
      " predictions [278826.94942952 634370.94971231 283465.28061587 ... 375873.7639295\n",
      " 383617.60292521 217249.18774508]\n",
      "errors [  56926.94942952   96370.94971231  103465.28061587 ...   15873.7639295\n",
      "  -16382.39707479 -107750.81225492]\n",
      "gradient 4.891760475632395e+21\n",
      "gradient magnitude 69941121492.52681\n",
      "\n",
      " predictions [278774.81627049 634510.21830376 283051.44941403 ... 375848.86111387\n",
      " 383630.66163577 217232.58583836]\n",
      "errors [  56874.81627049   96510.21830376  103051.44941403 ...   15848.86111387\n",
      "  -16369.33836423 -107767.41416164]\n",
      "gradient 4.6623875118228633e+21\n",
      "gradient magnitude 68281677716.81407\n",
      "\n",
      " predictions [278723.92003821 634646.18256732 282647.43689394 ... 375824.54914979\n",
      " 383643.41051147 217216.37783335]\n",
      "errors [  56823.92003821   96646.18256732  102647.43689394 ...   15824.54914979\n",
      "  -16356.58948853 -107783.62216665]\n",
      "gradient 4.4437697672843436e+21\n",
      "gradient magnitude 66661606395.91836\n",
      "\n",
      " predictions [278674.23138498 634778.92090244 282253.01009465 ... 375800.81401855\n",
      " 383655.85690353 217200.55438422]\n",
      "errors [  56774.23138498   96778.92090244  102253.01009465 ...   15800.81401855\n",
      "  -16344.14309647 -107799.44561578]\n",
      "gradient 4.2354029334896774e+21\n",
      "gradient magnitude 65079973367.30922\n",
      "\n",
      " predictions [278625.72165943 634908.50984847 281867.94158251 ... 375777.64203404\n",
      " 383668.00798877 217185.10636688]\n",
      "errors [  56725.72165943   96908.50984847  101867.94158251 ...   15777.64203404\n",
      "  -16331.99201123 -107814.89363312]\n",
      "gradient 4.0368063487647217e+21\n",
      "gradient magnitude 63535866632.6723\n",
      "\n",
      " predictions [278578.36288996 635035.02412873 281492.00932003 ... 375755.01983487\n",
      " 383679.87077373 217170.02487373]\n",
      "errors [  56678.36288996   97035.02412873  101492.00932003 ...   15755.01983487\n",
      "  -16320.12922627 -107829.97512627]\n",
      "gradient 3.847521889495722e+21\n",
      "gradient magnitude 62028395832.03584\n",
      "\n",
      " predictions [278532.12776866 635158.53669367 281124.99653783 ... 375732.93437668\n",
      " 383691.4520987  217155.30120851]\n",
      "errors [  56632.12776866   97158.53669367  101124.99653783 ...   15732.93437668\n",
      "  -16308.5479013  -107844.69879149]\n",
      "gradient 3.6671129133273836e+21\n",
      "gradient magnitude 60556691730.37265\n",
      "\n",
      " predictions [278486.98963551 635279.11876287 280766.69160968 ... 375711.37292458\n",
      " 383702.7586417  217140.92688128]\n",
      "errors [  56586.98963551   97279.11876287  100766.69160968 ...   15711.37292458\n",
      "  -16297.2413583  -107859.07311872]\n",
      "gradient 3.495163251914105e+21\n",
      "gradient magnitude 59119905716.38376\n",
      "\n",
      " predictions [278442.92246304 635396.83986614 280416.88793046 ... 375690.32304586\n",
      " 383713.79692226 217126.89360355]\n",
      "errors [  56542.92246304   97396.83986614  100416.88793046 ...   15690.32304586\n",
      "  -16286.20307774 -107873.10639645]\n",
      "gradient 3.33127625090057e+21\n",
      "gradient magnitude 57717209313.17253\n",
      "\n",
      " predictions [278399.90084134 635511.76788361 280075.38379702 ... 375669.77260276\n",
      " 383724.57330528 217113.19328347]\n",
      "errors [  56499.90084134   97511.76788361  100075.38379702 ...   15669.77260276\n",
      "  -16275.42669472 -107886.80671653]\n",
      "gradient 3.175073854917628e+21\n",
      "gradient magnitude 56347793700.53124\n",
      "\n",
      " predictions [278357.89996334 635623.96908485 279741.98229187 ... 375649.70974555\n",
      " 383735.09400461 217099.8180212 ]\n",
      "errors [  56457.89996334   97623.96908485   99741.98229187 ...   15649.70974555\n",
      "  -16264.90599539 -107900.1819788 ]\n",
      "gradient 3.0261957354822115e+21\n",
      "gradient magnitude 55010869248.56043\n",
      "\n",
      " predictions [278316.89561058 635733.50816712 279416.49116966 ... 375630.12290559\n",
      " 383745.36508667 217086.76010432]\n",
      "errors [  56416.89561058   97733.50816712   99416.49116966 ...   15630.12290559\n",
      "  -16254.63491333 -107913.23989568]\n",
      "gradient 2.884298459789656e+21\n",
      "gradient magnitude 53705665062.35312\n",
      "\n",
      " predictions [278276.86413922 635840.44829264 279098.72274629 ... 375611.00078878\n",
      " 383755.39247397 217074.01200339]\n",
      "errors [  56376.86413922   97840.44829264   99098.72274629 ...   15611.00078878\n",
      "  -16244.60752603 -107925.98799661]\n",
      "gradient 2.7490546984812644e+21\n",
      "gradient magnitude 52431428537.48374\n",
      "\n",
      " predictions [278237.78246636 635944.85112503 278788.49379073 ... 375592.33236896\n",
      " 383765.18194846 217061.56636763]\n",
      "errors [  56337.78246636   97944.85112503   98788.49379073 ...   15592.33236896\n",
      "  -16234.81805154 -107938.43363237]\n",
      "gradient 2.6201524705591296e+21\n",
      "gradient magnitude 51187424926.04145\n",
      "\n",
      " predictions [278199.62805683 636046.77686485 278485.62541931 ... 375574.10688156\n",
      " 383774.73915495 217049.41602066]\n",
      "errors [  56299.62805683   98046.77686485   98485.62541931 ...   15574.10688156\n",
      "  -16225.26084505 -107950.58397934]\n",
      "gradient 2.4972944237070296e+21\n",
      "gradient magnitude 49972936912.96349\n",
      "\n",
      " predictions [278162.37891008 636146.28428435 278189.94299266 ... 375556.31381745\n",
      " 383784.0696043  217037.55395638]\n",
      "errors [  56262.37891008   98146.28428435   98189.94299266 ...   15556.31381745\n",
      "  -16215.9303957  -107962.44604362]\n",
      "gradient 2.3801971483563666e+21\n",
      "gradient magnitude 48787264202.41625\n",
      "\n",
      " predictions [278126.01354759 636243.4307613  277901.2760149  ... 375538.94291682\n",
      " 383793.1786766  217025.9733349 ]\n",
      "errors [  56226.01354759   98243.4307613    97901.2760149  ...   15538.94291682\n",
      "  -16206.8213234  -107974.0266651 ]\n",
      "gradient 2.268590523915559e+21\n",
      "gradient magnitude 47629723113.99216\n",
      "\n",
      " predictions [278090.51100044 636338.27231213 277619.45803542 ... 375521.9841633\n",
      " 383802.07162432 217014.66747863]\n",
      "errors [  56190.51100044   98338.27231213   97619.45803542 ...   15521.9841633\n",
      "  -16197.92837568 -107985.33252137]\n",
      "gradient 2.1622170956545537e+21\n",
      "gradient magnitude 46499646188.48786\n",
      "\n",
      " predictions [278055.8507972  636430.86362419 277344.32655286 ... 375505.42777817\n",
      " 383810.7535753  217003.6298684 ]\n",
      "errors [  56155.8507972    98430.86362419   97344.32655286 ...   15505.42777817\n",
      "  -16189.2464247  -107996.3701316 ]\n",
      "gradient 2.06083148080699e+21\n",
      "gradient magnitude 45396381803.03569\n",
      "\n",
      " predictions [278022.01295218 636521.25808733 277075.72292141 ... 375489.2642147\n",
      " 383819.2295357  216992.85413973]\n",
      "errors [  56122.01295218   98521.25808733   97075.72292141 ...   15489.2642147\n",
      "  -16180.7704643  -108007.14586027]\n",
      "gradient 1.9641998025197822e+21\n",
      "gradient magnitude 44319293795.363914\n",
      "\n",
      " predictions [277988.97795386 636609.50782462 276813.49225934 ... 375473.4841527\n",
      " 383827.50439291 216982.33407915]\n",
      "errors [  56088.97795386   98609.50782462   96813.49225934 ...   15473.4841527\n",
      "  -16172.49560709 -108017.66592085]\n",
      "gradient 1.8720991503450057e+21\n",
      "gradient magnitude 43267761096.97618\n",
      "\n",
      " predictions [277956.72675366 636695.66372247 276557.48335968 ... 375458.07849311\n",
      " 383835.58291838 216972.06362059]\n",
      "errors [  56056.72675366   98695.66372247   96557.48335968 ...   15458.07849311\n",
      "  -16164.41708162 -108027.93637941]\n",
      "gradient 1.7843170660288975e+21\n",
      "gradient magnitude 42241177375.03179\n",
      "\n",
      " predictions [277925.24075496 636779.77545994 276307.54860307 ... 375443.03835272\n",
      " 383843.46977033 216962.03684193]\n",
      "errors [  56025.24075496   98779.77545994   96307.54860307 ...   15443.03835272\n",
      "  -16156.53022967 -108037.96315807]\n",
      "gradient 1.7006510534119355e+21\n",
      "gradient magnitude 41238950682.72149\n",
      "\n",
      " predictions [277894.50180236 636861.89153737 276063.54387258 ... 375428.35505914\n",
      " 383851.16949645 216952.24796154]\n",
      "errors [  55994.50180236   98861.89153737   96063.54387258 ...   15428.35505914\n",
      "  -16148.83050355 -108047.75203846]\n",
      "gradient 1.620908111309858e+21\n",
      "gradient magnitude 40260503117.942505\n",
      "\n",
      " predictions [277864.49217122 636942.05930439 275825.32847065 ... 375414.02014571\n",
      " 383858.68653656 216942.69133497]\n",
      "errors [  55964.49217122   98942.05930439   95825.32847065 ...   15414.02014571\n",
      "  -16141.31346344 -108057.30866503]\n",
      "gradient 1.544904288297416e+21\n",
      "gradient magnitude 39305270490.06808\n",
      "\n",
      " predictions [277835.19455745 637020.3249872  275592.76503796 ... 375400.02534664\n",
      " 383866.02522512 216933.36145171]\n",
      "errors [  55935.19455745   99020.3249872    95592.76503796 ...   15400.02534664\n",
      "  -16133.97477488 -108066.63854829]\n",
      "gradient 1.4724642583683594e+21\n",
      "gradient magnitude 38372701994.62581\n",
      "\n",
      " predictions [277806.5920675  637096.7337152  275365.71947422 ... 375386.3625923\n",
      " 383873.18979374 216924.25293197]\n",
      "errors [  55906.5920675    99096.7337152    95365.71947422 ...   15386.3625923\n",
      "  -16126.81020626 -108075.74706803]\n",
      "gradient 1.4034209164924357e+21\n",
      "gradient magnitude 37462259895.69283\n",
      "\n",
      " predictions [277778.66820867 637171.32954705 275144.06086083 ... 375373.02400449\n",
      " 383880.18437365 216915.36052362]\n",
      "errors [  55878.66820867   99171.32954705   95144.06086083 ...   15373.02400449\n",
      "  -16119.81562635 -108084.63947638]\n",
      "gradient 1.3376149931367884e+21\n",
      "gradient magnitude 36573419215.829254\n",
      "\n",
      " predictions [277751.40687956 637244.15549607 274927.6613854  ... 375360.00189194\n",
      " 383887.01299805 216906.67909914]\n",
      "errors [  55851.40687956   99244.15549607   94927.6613854  ...   15360.00189194\n",
      "  -16112.98700195 -108093.32090086]\n",
      "gradient 1.274894686862019e+21\n",
      "gradient magnitude 35705667433.364395\n",
      "\n",
      " predictions [277724.79236078 637315.25355502 274716.39626808 ... 375347.28874586\n",
      " 383893.67960445 216898.20365266]\n",
      "errors [  55824.79236078   99315.25355502   94716.39626808 ...   15347.28874586\n",
      "  -16106.32039555 -108101.79634734]\n",
      "gradient 1.215115314145979e+21\n",
      "gradient magnitude 34858504186.869225\n",
      "\n",
      " predictions [277698.80930592 637384.66472034 274510.14368955 ... 375334.87723563\n",
      " 383900.18803693 216889.92929708]\n",
      "errors [  55798.80930592   99384.66472034   94510.14368955 ...   15334.87723563\n",
      "  -16099.81196307 -108110.07070292]\n",
      "gradient 1.1581389756270222e+21\n",
      "gradient magnitude 34031440986.637962\n",
      "\n",
      " predictions [277673.44273269 637452.42901575 274308.78472085 ... 375322.76020453\n",
      " 383906.54204839 216881.85126127]\n",
      "errors [  55773.44273269   99452.42901575   94308.78472085 ...   15322.76020453\n",
      "  -16093.45795161 -108118.14873873]\n",
      "gradient 1.10383423799713e+21\n",
      "gradient magnitude 33224000933.017235\n",
      "\n",
      " predictions [277648.67801426 637518.58551539 274112.20325473 ... 375310.93066568\n",
      " 383912.74530265 216873.96488728]\n",
      "errors [  55748.67801426   99518.58551539   94112.20325473 ...   15310.93066568\n",
      "  -16087.25469735 -108126.03511272]\n",
      "gradient 1.0520758308107072e+21\n",
      "gradient magnitude 32435718441.414352\n",
      "\n",
      " predictions [277624.50087083 637583.17236629 273920.28593874 ... 375299.38179794\n",
      " 383918.80137663 216866.26562768]\n",
      "errors [  55724.50087083   99583.17236629   93920.28593874 ...   15299.38179794\n",
      "  -16081.19862337 -108133.73437232]\n",
      "gradient 1.0027443575103564e+21\n",
      "gradient magnitude 31666138973.836964\n",
      "\n",
      " predictions [277600.89736144 637646.2268104  273732.9221099  ... 375288.10694203\n",
      " 383924.71376237 216858.74904295]\n",
      "errors [  55700.89736144   99646.2268104    93732.9221099  ...   15288.10694203\n",
      "  -16075.28623763 -108141.25095705]\n",
      "gradient 9.55726020002013e+20\n",
      "gradient magnitude 30914818776.79397\n",
      "\n",
      " predictions [277577.85387587 637707.78520607 273550.00373079 ... 375277.09959667\n",
      " 383930.48586907 216851.41079888]\n",
      "errors [  55677.85387587   99707.78520607   93550.00373079 ...   15277.09959667\n",
      "  -16069.51413093 -108148.58920112]\n",
      "gradient 9.109123561451228e+20\n",
      "gradient magnitude 30181324625.422302\n",
      "\n",
      " predictions [277555.35712683 637767.88304897 273371.42532738 ... 375266.35341481\n",
      " 383936.12102501 216844.24666412]\n",
      "errors [  55655.35712683   99767.88304897   93371.42532738 ...   15266.35341481\n",
      "  -16063.87897499 -108155.75333588]\n",
      "gradient 8.681999895515039e+20\n",
      "gradient magnitude 29465233573.67974\n",
      "\n",
      " predictions [277533.39414229 637826.55499263 273197.0839281  ... 375255.86220002\n",
      " 383941.62247954 216837.2525077 ]\n",
      "errors [  55633.39414229   99826.55499263   93197.0839281  ...   15255.86220002\n",
      "  -16058.37752046 -108162.7474923 ]\n",
      "gradient 8.2749039111619e+20\n",
      "gradient magnitude 28766132710.466835\n",
      "\n",
      " predictions [277511.95225799 637883.83486834 273026.87900455 ... 375245.61990288\n",
      " 383946.9934049  216830.42429665]\n",
      "errors [  55611.95225799   99883.83486834   93026.87900455 ...   15245.61990288\n",
      "  -16053.0065951  -108169.57570335]\n",
      "gradient 7.886896517300853e+20\n",
      "gradient magnitude 28083618921.53654\n",
      "\n",
      " predictions [277491.01911016 637939.75570472 272860.71241348 ... 375235.62061748\n",
      " 383952.23689805 216823.75809371]\n",
      "errors [  55591.01911016   99939.75570472   92860.71241348 ...   15235.62061748\n",
      "  -16047.76310195 -108176.24190629]\n",
      "gradient 7.517082656498036e+20\n",
      "gradient magnitude 27417298657.048683\n",
      "\n",
      " predictions [277470.58262836 637994.34974675 272698.4883402  ... 375225.85857806\n",
      " 383957.3559825  216817.25005503]\n",
      "errors [  55570.58262836   99994.34974675   92698.4883402  ...   15225.85857806\n",
      "  -16042.6440175  -108182.74994497]\n",
      "gradient 7.164609240255127e+20\n",
      "gradient magnitude 26766787704.64459\n",
      "\n",
      " predictions [277450.63102854 638047.64847433 272540.11324338 ... 375216.32815566\n",
      " 383962.35361    216810.89642795]\n",
      "errors [  55550.63102854  100047.64847433   92540.11324338 ...   15216.32815566\n",
      "  -16037.64639    -108189.10357205]\n",
      "gradient 6.828663181098788e+20\n",
      "gradient magnitude 26131710967.90026\n",
      "\n",
      " predictions [277431.15280626 638099.68262049 272385.49580106 ... 375207.02385485\n",
      " 383967.23266228 216804.69354886]\n",
      "errors [  55531.15280626  100099.68262049   92385.49580106 ...   15207.02385485\n",
      "  -16032.76733772 -108195.30645114]\n",
      "gradient 6.508469516947789e+20\n",
      "gradient magnitude 25511702250.041626\n",
      "\n",
      " predictions [277412.13673002 638150.48218906 272234.54685802 ... 375197.94031061\n",
      " 383971.99595269 216798.63784107]\n",
      "errors [  55512.13673002  100150.48218906   92234.54685802 ...   15197.94031061\n",
      "  -16028.00404731 -108201.36215893]\n",
      "gradient 6.203289623425774e+20\n",
      "gradient magnitude 24906404042.78742\n",
      "\n",
      " predictions [277393.5718348  638200.07647198 272087.17937435 ... 375189.0722852\n",
      " 383976.64622782 216792.72581274]\n",
      "errors [  55493.5718348   100200.07647198   92087.17937435 ...   15189.0722852\n",
      "  -16023.35377218 -108207.27418726]\n",
      "gradient 5.91241950999935e+20\n",
      "gradient magnitude 24315467320.20454\n",
      "\n",
      " predictions [277375.44741575 638248.49406624 271943.30837529 ... 375180.41466517\n",
      " 383981.18616911 216786.95405491]\n",
      "errors [  55475.44741575  100248.49406624   91943.30837529 ...   15180.41466517\n",
      "  -16018.81383089 -108213.04594509]\n",
      "gradient 5.6351881960082244e+20\n",
      "gradient magnitude 23738551337.45154\n",
      "\n",
      " predictions [277357.75302199 638295.76289028 271802.8509022  ... 375171.96245836\n",
      " 383985.61839437 216781.31923946]\n",
      "errors [  55457.75302199  100295.76289028   91802.8509022  ...   15171.96245836\n",
      "  -16014.38160563 -108218.68076054]\n",
      "gradient 5.3709561628445304e+20\n",
      "gradient magnitude 23175323434.300827\n",
      "\n",
      " predictions [277340.47845064 638341.91020017 271665.72596477 ... 375163.71079109\n",
      " 383989.94545931 216775.81811726]\n",
      "errors [  55440.47845064  100341.91020017   91665.72596477 ...   15163.71079109\n",
      "  -16010.05454069 -108224.18188274]\n",
      "gradient 5.119113878707389e+20\n",
      "gradient magnitude 22625458843.319374\n",
      "\n",
      " predictions [277323.61374084 638386.96260529 271531.85449425 ... 375155.65490529\n",
      " 383994.16985899 216770.44751626]\n",
      "errors [  55423.61374084  100386.96260529   91531.85449425 ...   15155.65490529\n",
      "  -16005.83014101 -108229.55248374]\n",
      "gradient 4.8790803925327544e+20\n",
      "gradient magnitude 22088640502.603943\n",
      "\n",
      " predictions [277307.14916811 638430.94608367 271401.15929794 ... 375147.7901558\n",
      " 383998.29402928 216765.20433969]\n",
      "errors [  55407.14916811  100430.94608367   91401.15929794 ...   15147.7901558\n",
      "  -16001.70597072 -108234.79566031]\n",
      "gradient 4.650301993851034e+20\n",
      "gradient magnitude 21564558872.9541\n",
      "\n",
      " predictions [277291.07523867 638473.88599699 271273.5650146  ... 375140.11200766\n",
      " 384002.32034825 216760.08556423]\n",
      "errors [  55391.07523867  100473.88599699   91273.5650146  ...   15140.11200766\n",
      "  -15997.67965175 -108239.91443577]\n",
      "gradient 4.4322509354861776e+20\n",
      "gradient magnitude 21052911759.388954\n",
      "\n",
      " predictions [277275.38268402 638515.80710518 271148.99807106 ... 375132.6160335\n",
      " 384006.25113755 216755.0882383 ]\n",
      "errors [  55375.38268402  100515.80710518   91148.99807106 ...   15132.6160335\n",
      "  -15993.74886245 -108244.9117617 ]\n",
      "gradient 4.224424216145766e+20\n",
      "gradient magnitude 20553404136.896072\n",
      "\n",
      " predictions [277260.06245552 638556.73358072 271027.38663976 ... 375125.29791102\n",
      " 384010.08866375 216750.20948036]\n",
      "errors [  55360.06245552  100556.73358072   91027.38663976 ...   15125.29791102\n",
      "  -15989.91133625 -108249.79051964]\n",
      "gradient 4.0263424200943056e+20\n",
      "gradient magnitude 20065747980.31288\n",
      "\n",
      " predictions [277245.10571927 638596.68902254 270908.66059733 ... 375118.15342046\n",
      " 384013.83513962 216745.44647722]\n",
      "errors [  55345.10571927  100596.68902254   90908.66059733 ...   15118.15342046\n",
      "  -15986.16486038 -108254.55352278]\n",
      "gradient 3.8375486112351755e+20\n",
      "gradient magnitude 19589662098.247574\n",
      "\n",
      " predictions [277230.50385094 638635.69646969 270792.75148419 ... 375111.17844217\n",
      " 384017.49272547 216740.79648246]\n",
      "errors [  55330.50385094  100635.69646969   90792.75148419 ...   15111.17844217\n",
      "  -15982.50727453 -108259.20351754]\n",
      "gradient 3.657607279047979e+20\n",
      "gradient magnitude 19124871970.938732\n",
      "\n",
      " predictions [277216.24843083 638673.77841457 270679.59246503 ... 375104.36895425\n",
      " 384021.06353032 216736.2568148 ]\n",
      "errors [  55316.24843083  100673.77841457   90679.59246503 ...   15104.36895425\n",
      "  -15978.93646968 -108263.7431852 ]\n",
      "gradient 3.486103333950612e+20\n",
      "gradient magnitude 18671109591.962154\n",
      "\n",
      " predictions [277202.33123901 638710.95681592 270569.11829031 ... 375097.72103022\n",
      " 384024.54961316 216731.82485659]\n",
      "errors [  55302.33123901  100710.95681592   90569.11829031 ...   15097.72103022\n",
      "  -15975.45038684 -108268.17514341]\n",
      "gradient 3.3226411497685444e+20\n",
      "gradient magnitude 18228113313.693615\n",
      "\n",
      " predictions [277188.74425056 638747.25311148 270461.26525862 ... 375091.23083679\n",
      " 384027.95298413 216727.49805229]\n",
      "errors [  55288.74425056  100747.25311148   90461.26525862 ...   15091.23083679\n",
      "  -15972.04701587 -108272.50194771]\n",
      "gradient 3.166843651102199e+20\n",
      "gradient magnitude 17795627696.437683\n",
      "\n",
      " predictions [277175.47963099 638782.68823035 270355.97117994 ... 375084.89463158\n",
      " 384031.27560568 216723.27390698]\n",
      "errors [  55275.47963099  100782.68823035   90355.97117994 ...   15084.89463158\n",
      "  -15968.72439432 -108276.72609302]\n",
      "gradient 3.0183514434875084e+20\n",
      "gradient magnitude 17373403361.13655\n",
      "\n",
      " predictions [277162.52973168 638817.28260507 270253.17533979 ... 375078.70876102\n",
      " 384034.51939369 216719.14998494]\n",
      "errors [  55262.52973168  100817.28260507   90253.17533979 ...   15078.70876102\n",
      "  -15965.48060631 -108280.85001506]\n",
      "gradient 2.8768219843434178e+20\n",
      "gradient magnitude 16961196845.574955\n",
      "\n",
      " predictions [277149.88708549 638851.05618339 270152.81846423 ... 375072.66965824\n",
      " 384037.68621859 216715.12390824]\n",
      "errors [  55249.88708549  100851.05618339   90152.81846423 ...   15072.66965824\n",
      "  -15962.31378141 -108284.87609176]\n",
      "gradient 2.741928792792246e+20\n",
      "gradient magnitude 16558770463.993532\n",
      "\n",
      " predictions [277137.54440242 638884.02843975 270054.84268565 ... 375066.77384096\n",
      " 384040.77790644 216711.19335539]\n",
      "errors [  55237.54440242  100884.02843975   90054.84268565 ...   15066.77384096\n",
      "  -15959.22209356 -108288.80664461]\n",
      "gradient 2.6133606965320647e+20\n",
      "gradient magnitude 16165892170.035233\n",
      "\n",
      " predictions [277125.49456547 638916.21838657 269959.19150945 ... 375061.01790957\n",
      " 384043.79623995 216707.35605995]\n",
      "errors [  55225.49456547  100916.21838657   89959.19150945 ...   15061.01790957\n",
      "  -15956.20376005 -108292.64394005]\n",
      "gradient 2.4908211140233757e+20\n",
      "gradient magnitude 15782335422.944778\n",
      "\n",
      " predictions [277113.73062649 638947.64458515 269865.80978142 ... 375055.39854508\n",
      " 384046.74295954 216703.60980927]\n",
      "errors [  55213.73062649  100947.64458515   89865.80978142 ...   15055.39854508\n",
      "  -15953.25704046 -108296.39019073]\n",
      "gradient 2.374027370332881e+20\n",
      "gradient magnitude 15407879056.93993\n",
      "\n",
      " predictions [277102.24580218 638978.3251564  269774.64365597 ... 375049.91250727\n",
      " 384049.61976437 216699.95244319]\n",
      "errors [  55202.24580218  100978.3251564    89774.64365597 ...   15049.91250727\n",
      "  -15950.38023563 -108300.04755681]\n",
      "gradient 2.262710045057075e+20\n",
      "gradient magnitude 15042307153.681828\n",
      "\n",
      " predictions [277091.03347017 639008.27779132 269685.64056503 ... 375044.55663279\n",
      " 384052.42831323 216696.38185281]\n",
      "errors [  55191.03347017  101008.27779132   89685.64056503 ...   15044.55663279\n",
      "  -15947.57168677 -108303.61814719]\n",
      "gradient 2.156612350822997e+20\n",
      "gradient magnitude 14685408917.776165\n",
      "\n",
      " predictions [277080.08716524 639037.51976111 269598.74918783 ... 375039.32783334\n",
      " 384055.1702256  216692.89597927]\n",
      "errors [  55180.08716524  101037.51976111   89598.74918783 ...   15039.32783334\n",
      "  -15944.8297744  -108307.10402073]\n",
      "gradient 2.0554895409287515e+20\n",
      "gradient magnitude 14336978555.22129\n",
      "\n",
      " predictions [277069.40057555 639066.06792725 269513.91942121 ... 375034.22309391\n",
      " 384057.84708251 216689.49281254]\n",
      "errors [  55169.40057555  101066.06792725   89513.91942121 ...   15034.22309391\n",
      "  -15942.15291749 -108310.50718746]\n",
      "gradient 1.9591083447610687e+20\n",
      "gradient magnitude 13996815154.74527\n",
      "\n",
      " predictions [277058.96753901 639093.93875111 269431.10235079 ... 375029.23947102\n",
      " 384060.46042749 216686.17039031]\n",
      "errors [  55158.96753901  101093.93875111   89431.10235079 ...   15029.23947102\n",
      "  -15939.53957251 -108313.82960969]\n",
      "gradient 1.8672464296856276e+20\n",
      "gradient magnitude 13664722571.957426\n",
      "\n",
      " predictions [277048.78203976 639121.14830353 269350.25022275 ... 375024.37409101\n",
      " 384063.01176744 216682.9267968 ]\n",
      "errors [  55148.78203976  101121.14830353   89350.25022275 ...   15024.37409101\n",
      "  -15936.98823256 -108317.0732032 ]\n",
      "gradient 1.7796918881683528e+20\n",
      "gradient magnitude 13340509316.245586\n",
      "\n",
      " predictions [277038.83820464 639147.71227402 269271.31641627 ... 375019.62414841\n",
      " 384065.5025735  216679.76016171]\n",
      "errors [  55138.83820464  101147.71227402   89271.31641627 ...   15019.62414841\n",
      "  -15934.4974265  -108320.23983829]\n",
      "gradient 1.6962427489468275e+20\n",
      "gradient magnitude 13023988440.36199\n",
      "\n",
      " predictions [277029.13029986 639173.64597985 269194.25541669 ... 375014.98690434\n",
      " 384067.93428193 216676.66865909]\n",
      "errors [  55129.13029986  101173.64597985   89194.25541669 ...   15014.98690434\n",
      "  -15932.06571807 -108323.33134091]\n",
      "gradient 1.616706511121333e+20\n",
      "gradient magnitude 12714977432.62383\n",
      "\n",
      " predictions [277019.65272767 639198.96437488 269119.02278925 ... 375010.45968486\n",
      " 384070.30829489 216673.65050632]\n",
      "errors [  55119.65272767  101198.96437488   89119.02278925 ...   15010.45968486\n",
      "  -15929.69170511 -108326.34949368]\n",
      "gradient 1.540899700093178e+20\n",
      "gradient magnitude 12413298111.675148\n",
      "\n",
      " predictions [277010.40002313 639223.68205814 269045.57515343 ... 375006.0398795\n",
      " 384072.62598128 216670.70396309]\n",
      "errors [  55110.40002313  101223.68205814   89045.57515343 ...   15006.0398795\n",
      "  -15927.37401872 -108329.29603691]\n",
      "gradient 1.4686474443243586e+20\n",
      "gradient magnitude 12118776523.74347\n",
      "\n",
      " predictions [277001.36685096 639247.8132823  268973.87015801 ... 375001.72493973\n",
      " 384074.88867752 216667.82733037]\n",
      "errors [  55101.36685096  101247.8132823    88973.87015801 ...   15001.72493973\n",
      "  -15925.11132248 -108332.17266963]\n",
      "gradient 1.3997830719428936e+20\n",
      "gradient magnitude 11831242842.334415\n",
      "\n",
      " predictions [276992.54800248 639271.37196186 268903.86645657 ... 374997.51237747\n",
      " 384077.09768833 216665.01894944]\n",
      "errors [  55092.54800248  101271.37196186   88903.86645657 ...   14997.51237747\n",
      "  -15922.90231167 -108334.98105056]\n",
      "gradient 1.3341477262637025e+20\n",
      "gradient magnitude 11550531270.308317\n",
      "\n",
      " predictions [276983.93839257 639294.37168119 268835.52368372 ... 374993.39976368\n",
      " 384079.25428746 216662.27720094]\n",
      "errors [  55083.93839257  101294.37168119   88835.52368372 ...   14993.39976368\n",
      "  -15920.74571254 -108337.72279906]\n",
      "gradient 1.271589999337037e+20\n",
      "gradient magnitude 11276479944.277988\n",
      "\n",
      " predictions [276975.53305678 639316.82570234 268768.80243177 ... 374989.38472695\n",
      " 384081.35971845 216659.60050392]\n",
      "errors [  55075.53305678  101316.82570234   88768.80243177 ...   14989.38472695\n",
      "  -15918.64028155 -108340.39949608]\n",
      "gradient 1.2119655826794404e+20\n",
      "gradient magnitude 11008930841.273554\n",
      "\n",
      " predictions [276967.32714844 639338.74697271 268703.66422803 ... 374985.46495215\n",
      " 384083.41519532 216656.98731496]\n",
      "errors [  55067.32714844  101338.74697271   88703.66422803 ...   14985.46495215\n",
      "  -15916.58480468 -108343.01268504]\n",
      "gradient 1.1551369343826806e+20\n",
      "gradient magnitude 10747729687.625572\n",
      "\n",
      " predictions [276959.31593588 639360.14813251 268640.07151264 ... 374981.63817906\n",
      " 384085.42190331 216654.43612725]\n",
      "errors [  55059.31593588  101360.14813251   88640.07151264 ...   14981.63817906\n",
      "  -15914.57809669 -108345.56387275]\n",
      "gradient 1.1009729618313714e+20\n",
      "gradient magnitude 10492725870.008095\n",
      "\n",
      " predictions [276951.49479969 639381.04152202 268577.98761687 ... 374977.90220109\n",
      " 384087.38099951 216651.94546972]\n",
      "errors [  55051.49479969  101381.04152202   88577.98761687 ...   14977.90220109\n",
      "  -15912.61900049 -108348.05453028]\n",
      "gradient 1.0493487192974718e+20\n",
      "gradient magnitude 10243772348.590492\n",
      "\n",
      " predictions [276943.85923005 639401.43918876 268517.37674204 ... 374974.25486401\n",
      " 384089.29361359 216649.51390623]\n",
      "errors [  55043.85923005  101401.43918876   88517.37674204 ...   14974.25486401\n",
      "  -15910.70638641 -108350.48609377]\n",
      "gradient 1.0001451197152241e+20\n",
      "gradient magnitude 10000725572.253366\n",
      "\n",
      " predictions [276936.40482416 639421.3528944  268458.20393879 ... 374970.69406471\n",
      " 384091.16084838 216647.14003467]\n",
      "errors [  55036.40482416  101421.3528944    88458.20393879 ...   14970.69406471\n",
      "  -15908.83915162 -108352.85996533]\n",
      "gradient 9.532486599704114e+19\n",
      "gradient magnitude 9763445395.813976\n",
      "\n",
      " predictions [276929.12728368 639440.79412153 268400.43508703 ... 374967.21774995\n",
      " 384092.98378057 216644.82248625]\n",
      "errors [  55029.12728368  101440.79412153   88400.43508703 ...   14967.21774995\n",
      "  -15907.01621943 -108355.17751375]\n",
      "gradient 9.085511590707044e+19\n",
      "gradient magnitude 9531794999.215544\n",
      "\n",
      " predictions [276922.02241225 639459.77408032 268344.03687619 ... 374963.82391524\n",
      " 384094.7634613  216642.55992462]\n",
      "errors [  55022.02241225  101459.77408032   88344.03687619 ...   14963.82391524\n",
      "  -15905.2365387  -108357.44007538]\n",
      "gradient 8.659495085932143e+19\n",
      "gradient magnitude 9305640808.634375\n",
      "\n",
      " predictions [276915.08611306 639478.30371497 268288.97678602 ... 374960.51060363\n",
      " 384096.50091676 216640.35104515]\n",
      "errors [  55015.08611306  101478.30371497   88288.97678602 ...   14960.51060363\n",
      "  -15903.49908324 -108359.64895485]\n",
      "gradient 8.253454348334583e+19\n",
      "gradient magnitude 9084852419.458769\n",
      "\n",
      " predictions [276908.31438653 639496.39370999 268235.22306789 ... 374957.2759046\n",
      " 384098.19714881 216638.19457415]\n",
      "errors [  55008.31438653  101496.39370999   88235.22306789 ...   14957.2759046\n",
      "  -15901.80285119 -108361.80542585]\n",
      "gradient 7.866452721072777e+19\n",
      "gradient magnitude 8869302521.096445\n",
      "\n",
      " predictions [276901.70332795 639514.05449641 268182.74472643 ... 374954.11795297\n",
      " 384099.85313552 216636.08926818]\n",
      "errors [  55001.70332795  101514.05449641   88182.74472643 ...   14954.11795297\n",
      "  -15900.14686448 -108363.91073182]\n",
      "gradient 7.49759746682007e+19\n",
      "gradient magnitude 8658866823.563042\n",
      "\n",
      " predictions [276895.24912527 639531.29625776 268131.51150166 ... 374951.03492781\n",
      " 384101.46983176 216634.03391327]\n",
      "errors [  54995.24912527  101531.29625776   88131.51150166 ...   14951.03492781\n",
      "  -15898.53016824 -108365.96608673]\n",
      "gradient 7.146037708395951e+19\n",
      "gradient magnitude 8453423985.815423\n",
      "\n",
      " predictions [276888.94805687 639548.12893594 268081.49385158 ... 374948.02505139\n",
      " 384103.04816976 216632.02732427]\n",
      "errors [  54988.94805687  101548.12893594   88081.49385158 ...   14948.02505139\n",
      "  -15896.95183024 -108367.97267573]\n",
      "gradient 6.810962465957394e+19\n",
      "gradient magnitude 8252855545.783771\n",
      "\n",
      " predictions [276882.79648946 639564.56223699 268032.66293511 ... 374945.08658816\n",
      " 384104.58905961 216630.06834414]\n",
      "errors [  54982.79648946  101564.56223699   88032.66293511 ...   14945.08658816\n",
      "  -15895.41094039 -108369.93165586]\n",
      "gradient 6.491598786228812e+19\n",
      "gradient magnitude 8057045852.0656395\n",
      "\n",
      " predictions [276876.79087592 639580.60563666 267984.99059545 ... 374942.21784375\n",
      " 384106.09338981 216628.15584331]\n",
      "errors [  54976.79087592  101580.60563666   87984.99059545 ...   14942.21784375\n",
      "  -15893.90661019 -108371.84415669]\n",
      "gradient 6.18720995944533e+19\n",
      "gradient magnitude 7865881997.236756\n",
      "\n",
      " predictions [276870.92775331 639596.26838584 267938.44934387 ... 374939.41716399\n",
      " 384107.5620278  216626.28871899]\n",
      "errors [  54970.92775331  101596.26838584   87938.44934387 ...   14939.41716399\n",
      "  -15892.4379722  -108373.71128101]\n",
      "gradient 5.897093819910715e+19\n",
      "gradient magnitude 7679253752.74884\n",
      "\n",
      " predictions [276865.20374085 639611.55951598 267893.01234383 ... 374936.68293397\n",
      " 384108.99582041 216624.46589456]\n",
      "errors [  54965.20374085  101611.55951598   87893.01234383 ...   14936.68293397\n",
      "  -15891.00417959 -108375.53410544]\n",
      "gradient 5.6205811262350156e+19\n",
      "gradient magnitude 7497053505.3679695\n",
      "\n",
      " predictions [276859.61553796 639626.48784421 267848.65339554 ... 374934.01357706\n",
      " 384110.39559441 216622.68631896]\n",
      "errors [  54959.61553796  101626.48784421   87848.65339554 ...   14934.01357706\n",
      "  -15889.60440559 -108377.31368104]\n",
      "gradient 5.357034017527289e+19\n",
      "gradient magnitude 7319176195.124208\n",
      "\n",
      " predictions [276854.1599224  639641.06197848 267805.34692082 ... 374931.40755408\n",
      " 384111.76215691 216620.94896604]\n",
      "errors [  54954.1599224   101641.06197848   87805.34692082 ...   14931.40755408\n",
      "  -15888.23784309 -108379.05103396]\n",
      "gradient 5.1058445419750244e+19\n",
      "gradient magnitude 7145519254.732314\n",
      "\n",
      " predictions [276848.83374835 639655.2903225  267763.06794837 ... 374928.86336234\n",
      " 384113.09629592 216619.25283402]\n",
      "errors [  54948.83374835  101655.2903225    87763.06794837 ...   14928.86336234\n",
      "  -15886.90370408 -108380.74716598]\n",
      "gradient 4.8664332544151765e+19\n",
      "gradient magnitude 6975982550.447769\n",
      "\n",
      " predictions [276843.63394465 639669.1810806  267721.79209936 ... 374926.37953482\n",
      " 384114.39878071 216617.59694489]\n",
      "errors [  54943.63394465  101669.1810806    87721.79209936 ...   14926.37953482\n",
      "  -15885.60121929 -108382.40305511]\n",
      "gradient 4.6382478796694725e+19\n",
      "gradient magnitude 6810468324.329446\n",
      "\n",
      " predictions [276838.55751299 639682.74226242 267681.4955734  ... 374923.9546393\n",
      " 384115.67036233 216615.98034382]\n",
      "errors [  54938.55751299  101682.74226242   87681.4955734  ...   14923.9546393\n",
      "  -15884.32963767 -108384.01965618]\n",
      "gradient 4.420762038552966e+19\n",
      "gradient magnitude 6648881137.870465\n",
      "\n",
      " predictions [276833.60152622 639695.98168761 267642.15513477 ... 374921.58727754\n",
      " 384116.91177399 216614.40209866]\n",
      "errors [  54933.60152622  101695.98168761   87642.15513477 ...   14921.58727754\n",
      "  -15883.08822601 -108385.59790134]\n",
      "gradient 4.213474033618385e+19\n",
      "gradient magnitude 6491127816.965543\n",
      "\n",
      " predictions [276828.76312663 639708.90699026 267603.74809907 ... 374919.27608448\n",
      " 384118.12373152 216612.86129936]\n",
      "errors [  54928.76312663  101708.90699026   87603.74809907 ...   14919.27608448\n",
      "  -15881.87626848 -108387.13870064]\n",
      "gradient 4.015905691839932e+19\n",
      "gradient magnitude 6337117398.186602\n",
      "\n",
      " predictions [276824.0395243  639721.52562332 267566.25232009 ... 374917.01972744\n",
      " 384119.30693376 216611.35705747]\n",
      "errors [  54924.0395243   101721.52562332   87566.25232009 ...   14917.01972744\n",
      "  -15880.69306624 -108388.64294253]\n",
      "gradient 3.827601261560143e+19\n",
      "gradient magnitude 6186761076.330767\n",
      "\n",
      " predictions [276819.42799552 639733.84486293 267529.6461771  ... 374914.81690537\n",
      " 384120.46206295 216609.88850562]\n",
      "errors [  54919.42799552  101733.84486293   87529.6461771  ...   14914.81690537\n",
      "  -15879.53793705 -108390.11149438]\n",
      "gradient 3.64812636115861e+19\n",
      "gradient magnitude 6039972153.212803\n",
      "\n",
      " predictions [276814.9258812  639745.87181259 267493.90856232 ... 374912.66634808\n",
      " 384121.58978517 216608.45479702]\n",
      "errors [  54914.9258812   101745.87181259   87493.90856232 ...   14912.66634808\n",
      "  -15878.41021483 -108391.54520298]\n",
      "gradient 3.4770669770214433e+19\n",
      "gradient magnitude 5896665987.675954\n",
      "\n",
      " predictions [276810.53058535 639757.61340726 267459.01886879 ... 374910.56681552\n",
      " 384122.69075069 216607.05510496]\n",
      "errors [  54910.53058535  101757.61340726   87459.01886879 ...   14910.56681552\n",
      "  -15877.30924931 -108392.94489504]\n",
      "gradient 3.31402850848912e+19\n",
      "gradient magnitude 5756759946.783538\n",
      "\n",
      " predictions [276806.23957355 639769.07641736 267424.95697849 ... 374908.51709706\n",
      " 384123.76559434 216605.68862236]\n",
      "errors [  54906.23957355  101769.07641736   87424.95697849 ...   14908.51709706\n",
      "  -15876.23440566 -108394.31137764]\n",
      "gradient 3.1586348575917556e+19\n",
      "gradient magnitude 5620173358.172998\n",
      "\n",
      " predictions [276802.05037152 639780.26745266 267391.70325071 ... 374906.5160108\n",
      " 384124.81493589 216604.35456128]\n",
      "errors [  54902.05037152  101780.26745266   87391.70325071 ...   14906.5160108\n",
      "  -15875.18506411 -108395.64543872]\n",
      "gradient 3.0105275614633128e+19\n",
      "gradient magnitude 5486827463.537844\n",
      "\n",
      " predictions [276797.96056372 639791.19296611 267359.23851075 ... 374904.56240288\n",
      " 384125.83938043 216603.05215248]\n",
      "errors [  54897.96056372  101791.19296611   87359.23851075 ...   14904.56240288\n",
      "  -15874.16061957 -108396.94784752]\n",
      "gradient 2.869364965438582e+19\n",
      "gradient magnitude 5356645373.215014\n",
      "\n",
      " predictions [276793.96779186 639801.85925758 267327.54403885 ... 374902.65514681\n",
      " 384126.83951865 216601.78064496]\n",
      "errors [  54893.96779186  101801.85925758   87327.54403885 ...   14902.65514681\n",
      "  -15873.16048135 -108398.21935504]\n",
      "gradient 2.734821434921227e+19\n",
      "gradient magnitude 5229552021.847786\n",
      "\n",
      " predictions [276790.06975366 639812.27247744 267296.6015594  ... 374900.79314283\n",
      " 384127.81592727 216600.53930557]\n",
      "errors [  54890.06975366  101812.27247744   87296.6015594  ...   14900.79314283\n",
      "  -15872.18407273 -108399.46069443]\n",
      "gradient 2.606586604209357e+19\n",
      "gradient magnitude 5105474125.102738\n",
      "\n",
      " predictions [276786.26420143 639822.43863013 267266.3932304  ... 374898.97531729\n",
      " 384128.76916929 216599.3274185 ]\n",
      "errors [  54886.26420143  101822.43863013   87266.3932304  ...   14898.97531729\n",
      "  -15871.23083071 -108400.6725815 ]\n",
      "gradient 2.4843646605422744e+19\n",
      "gradient magnitude 4984340137.412649\n",
      "\n",
      " predictions [276782.54894082 639832.36357765 267236.90163318 ... 374897.20062198\n",
      " 384129.69979438 216598.14428498]\n",
      "errors [  54882.54894082  101832.36357765   87236.90163318 ...   14897.20062198\n",
      "  -15870.30020562 -108401.85571502]\n",
      "gradient 2.3678736617177383e+19\n",
      "gradient magnitude 4866080210.721704\n",
      "\n",
      " predictions [276778.92182956 639842.0530429  267208.10976236 ... 374895.46803359\n",
      " 384130.60833914 216596.98922279]\n",
      "errors [  54878.92182956  101842.0530429    87208.10976236 ...   14895.46803359\n",
      "  -15869.39166086 -108403.01077721]\n",
      "gradient 2.2568448857052905e+19\n",
      "gradient magnitude 4750626154.208822\n",
      "\n",
      " predictions [276775.38077617 639851.51261301 267180.00101602 ... 374893.77655307\n",
      " 384131.49532747 216595.86156589]\n",
      "errors [  54875.38077617  101851.51261301   87180.00101602 ...   14893.77655307\n",
      "  -15868.50467253 -108404.13843411]\n",
      "gradient 2.1510222107587924e+19\n",
      "gradient magnitude 4637911394.969499\n",
      "\n",
      " predictions [276771.92373882 639860.74774254 267152.55918614 ... 374892.1252051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 384132.36127082 216594.76066407]\n",
      "errors [  54871.92373882  101860.74774254   87152.55918614 ...   14892.1252051\n",
      "  -15867.63872918 -108405.23933593]\n",
      "gradient 2.0501615245926294e+19\n",
      "gradient magnitude 4527870939.627839\n",
      "\n",
      " predictions [276768.54872413 639869.76375662 267125.76844926 ... 374890.51303747\n",
      " 384133.20666851 216593.68588251]\n",
      "errors [  54868.54872413  101869.76375662   87125.76844926 ...   14890.51303747\n",
      "  -15866.79333149 -108406.31411749]\n",
      "gradient 1.9540301612614025e+19\n",
      "gradient magnitude 4420441336.859254\n",
      "\n",
      " predictions [276765.25378601 639878.56585406 267099.61335736 ... 374888.93912057\n",
      " 384134.03200801 216592.6366015 ]\n",
      "errors [  54865.25378601  101878.56585406   87099.61335736 ...   14888.93912057\n",
      "  -15865.96799199 -108407.3633985 ]\n",
      "gradient 1.862406364447247e+19\n",
      "gradient magnitude 4315560640.805836\n",
      "\n",
      " predictions [276762.03702452 639887.15911031 267074.07882894 ... 374887.40254687\n",
      " 384134.83776522 216591.61221598]\n",
      "errors [  54862.03702452  101887.15911031   87074.07882894 ...   14887.40254687\n",
      "  -15865.16223478 -108408.38778402]\n",
      "gradient 1.7750787759078832e+19\n",
      "gradient magnitude 4213168375.353498\n",
      "\n",
      " predictions [276758.89658483 639895.54848039 267049.15014033 ... 374885.90243034\n",
      " 384135.62440477 216590.61213529]\n",
      "errors [  54858.89658483  101895.54848039   87049.15014033 ...   14885.90243034\n",
      "  -15864.37559523 -108409.38786471]\n",
      "gradient 1.6918459479173366e+19\n",
      "gradient magnitude 4113205499.263727\n",
      "\n",
      " predictions [276755.83065611 639903.73880177 267024.81291719 ... 374884.43790599\n",
      " 384136.39238024 216589.63578277]\n",
      "errors [  54855.83065611  101903.73880177   87024.81291719 ...   14884.43790599\n",
      "  -15863.60761976 -108410.36421723]\n",
      "gradient 1.6125158785633495e+19\n",
      "gradient magnitude 4015614372.1270714\n",
      "\n",
      " predictions [276752.83747049 639911.73479714 267001.05312623 ... 374883.00812935\n",
      " 384137.14213446 216588.68259542]\n",
      "errors [  54852.83747049  101911.73479714   87001.05312623 ...   14883.00812935\n",
      "  -15862.85786554 -108411.31740458]\n",
      "gradient 1.5369055688382906e+19\n",
      "gradient magnitude 3920338721.1289415\n",
      "\n",
      " predictions [276749.91530204 639919.54107713 266977.85706714 ... 374881.61227598\n",
      " 384137.87409976 216587.75202362]\n",
      "errors [  54849.91530204  101919.54107713   86977.85706714 ...   14881.61227598\n",
      "  -15862.12590024 -108412.24797638]\n",
      "gradient 1.464840600494195e+19\n",
      "gradient magnitude 3827323608.59935\n",
      "\n",
      " predictions [276747.06246578 639927.16214299 266955.21136463 ... 374880.24954102\n",
      " 384138.5886982  216586.8435308 ]\n",
      "errors [  54847.06246578  101927.16214299   86955.21136463 ...   14880.24954102\n",
      "  -15861.4113018  -108413.1564692 ]\n",
      "gradient 1.396154733694584e+19\n",
      "gradient magnitude 3736515400.3356977\n",
      "\n",
      " predictions [276744.27731673 639934.60238916 266933.10296079 ... 374878.91913868\n",
      " 384139.28634183 216585.9565931 ]\n",
      "errors [  54844.27731673  101934.60238916   86933.10296079 ...   14878.91913868\n",
      "  -15860.71365817 -108414.0434069 ]\n",
      "gradient 1.3306895235322016e+19\n",
      "gradient magnitude 3647861734.6771817\n",
      "\n",
      " predictions [276741.55824891 639941.86610582 266911.5191075  ... 374877.62030184\n",
      " 384139.96743293 216585.09069909]\n",
      "errors [  54841.55824891  101941.86610582   86911.5191075  ...   14877.62030184\n",
      "  -15860.03256707 -108414.90930091]\n",
      "gradient 1.2682939545263124e+19\n",
      "gradient magnitude 3561311492.3105397\n",
      "\n",
      " predictions [276738.90369446 639948.95748137 266890.44735914 ... 374876.35228155\n",
      " 384140.63236422 216584.2453495 ]\n",
      "errors [  54838.90369446  101948.95748137   86890.44735914 ...   14876.35228155\n",
      "  -15859.36763578 -108415.7546505 ]\n",
      "gradient 1.2088240922604143e+19\n",
      "gradient magnitude 3476814766.7950535\n",
      "\n",
      " predictions [276736.31212273 639955.88060482 266869.87556533 ... 374875.11434666\n",
      " 384141.28151913 216583.42005687]\n",
      "errors [  54836.31212273  101955.88060482   86869.87556533 ...   14875.11434666\n",
      "  -15858.71848087 -108416.57994313]\n",
      "gradient 1.1521427513530968e+19\n",
      "gradient magnitude 3394322835.7849183\n",
      "\n",
      " predictions [276733.78203935 639962.63946817 266849.79186402 ... 374873.90578335\n",
      " 384141.91527196 216582.61434532]\n",
      "errors [  54833.78203935  101962.63946817   86849.79186402 ...   14873.90578335\n",
      "  -15858.08472804 -108417.38565468]\n",
      "gradient 1.098119178998588e+19\n",
      "gradient magnitude 3313788132.9357615\n",
      "\n",
      " predictions [276731.31198544 639969.23796871 266830.18467458 ... 374872.72589474\n",
      " 384142.53398815 216581.82775028]\n",
      "errors [  54831.31198544  101969.23796871   86830.18467458 ...   14872.72589474\n",
      "  -15857.46601185 -108418.17224972]\n",
      "gradient 1.0466287533447623e+19\n",
      "gradient magnitude 3235164220.475929\n",
      "\n",
      " predictions [276728.90053673 639975.67991124 266811.04269114 ... 374871.57400049\n",
      " 384143.13802446 216581.05981817]\n",
      "errors [  54828.90053673  101975.67991124   86811.04269114 ...   14871.57400049\n",
      "  -15856.86197554 -108418.94018183]\n",
      "gradient 9.975526960149322e+18\n",
      "gradient magnitude 3158405762.4297295\n",
      "\n",
      " predictions [276726.54630273 639981.96901032 266792.35487609 ... 374870.44943639\n",
      " 384143.72772919 216580.3101062 ]\n",
      "errors [  54826.54630273  101981.96901032   86792.35487609 ...   14870.44943639\n",
      "  -15856.27227081 -108419.6898938 ]\n",
      "gradient 9.507777981105066e+18\n",
      "gradient magnitude 3083468498.477821\n",
      "\n",
      " predictions [276724.24792595 639988.10889235 266774.11045369 ... 374869.35155401\n",
      " 384144.30344238 216579.57818207]\n",
      "errors [  54824.24792595  101988.10889235   86774.11045369 ...   14869.35155401\n",
      "  -15855.69655762 -108420.42181793]\n",
      "gradient 9.061961590582237e+18\n",
      "gradient magnitude 3010309218.4329233\n",
      "\n",
      " predictions [276722.00408109 639994.10309769 266756.29890389 ... 374868.27972028\n",
      " 384144.865496   216578.86362373]\n",
      "errors [  54822.00408109  101994.10309769   86756.29890389 ...   14868.27972028\n",
      "  -15855.134504   -108421.13637627]\n",
      "gradient 8.637049377059414e+18\n",
      "gradient magnitude 2938885737.3262086\n",
      "\n",
      " predictions [276719.81347433 639999.95508273 266738.90995621 ... 374867.23331717\n",
      " 384145.41421413 216578.16601917]\n",
      "errors [  54819.81347433  101999.95508273   86738.90995621 ...   14867.23331717\n",
      "  -15854.58578587 -108421.83398083]\n",
      "gradient 8.232061150880997e+18\n",
      "gradient magnitude 2869156871.0826874\n",
      "\n",
      " predictions [276717.67484252 640005.66822181 266721.93358387 ... 374866.2117413\n",
      " 384145.94991318 216577.48496613]\n",
      "errors [  54817.67484252  102005.66822181   86721.93358387 ...   14866.2117413\n",
      "  -15854.05008682 -108422.51503387]\n",
      "gradient 7.846062683136743e+18\n",
      "gradient magnitude 2801082412.77131\n",
      "\n",
      " predictions [276715.58695248 640011.24580924 266705.359998   ... 374865.21440361\n",
      " 384146.47290204 216576.82007191]\n",
      "errors [  54815.58695248  102011.24580924   86705.359998   ...   14865.21440361\n",
      "  -15853.52709796 -108423.17992809]\n",
      "gradient 7.478163550598149e+18\n",
      "gradient magnitude 2734623109.424432\n",
      "\n",
      " predictions [276713.5486003  640016.69106115 266689.17964195 ... 374864.24072902\n",
      " 384146.98348228 216576.17095311]\n",
      "errors [  54813.5486003   102016.69106115   86689.17964195 ...   14864.24072902\n",
      "  -15853.01651772 -108423.82904689]\n",
      "gradient 7.12751508167023e+18\n",
      "gradient magnitude 2669740639.4011817\n",
      "\n",
      " predictions [276711.55861063 640022.00711739 266673.38318583 ... 374863.2901561\n",
      " 384147.4819483  216575.53723545]\n",
      "errors [  54811.55861063  102022.00711739   86673.38318583 ...   14863.2901561\n",
      "  -15852.5180517  -108424.46276455]\n",
      "gradient 6.793308398679382e+18\n",
      "gradient magnitude 2606397590.291892\n",
      "\n",
      " predictions [276709.61583602 640027.19704328 266657.96152114 ... 374862.36213672\n",
      " 384147.96858754 216574.91855351]\n",
      "errors [  54809.61583602  102027.19704328   86657.96152114 ...   14862.36213672\n",
      "  -15852.03141246 -108425.08144649]\n",
      "gradient 6.474772551940473e+18\n",
      "gradient magnitude 2544557437.3435693\n",
      "\n",
      " predictions [276707.71915621 640032.26383142 266642.90575545 ... 374861.45613578\n",
      " 384148.4436806  216574.31455056]\n",
      "errors [  54807.71915621  102032.26383142   86642.90575545 ...   14861.45613578\n",
      "  -15851.5563194  -108425.68544944]\n",
      "gradient 6.171172741345987e+18\n",
      "gradient magnitude 2484184522.402872\n",
      "\n",
      " predictions [276705.86747755 640037.21040344 266628.20720733 ... 374860.57163085\n",
      " 384148.90750143 216573.7248783 ]\n",
      "errors [  54805.86747755  102037.21040344   86628.20720733 ...   14860.57163085\n",
      "  -15851.09249857 -108426.2751217 ]\n",
      "gradient 5.881808621302356e+18\n",
      "gradient magnitude 2425244033.350532\n",
      "\n",
      " predictions [276704.05973233 640042.03961159 266613.85740134 ... 374859.70811192\n",
      " 384149.36031748 216573.14919674]\n",
      "errors [  54804.05973233  102042.03961159   86613.85740134 ...   14859.70811192\n",
      "  -15850.63968252 -108426.85080326]\n",
      "gradient 5.606012685183894e+18\n",
      "gradient magnitude 2367701984.030907\n",
      "\n",
      " predictions [276702.29487817 640046.7542405  266599.84806312 ... 374858.86508107\n",
      " 384149.80238984 216572.58717392]\n",
      "errors [  54802.29487817  102046.7542405    86599.84806312 ...   14858.86508107\n",
      "  -15850.19761016 -108427.41282608]\n",
      "gradient 5.343148725518183e+18\n",
      "gradient magnitude 2311525194.653561\n",
      "\n",
      " predictions [276700.57189742 640051.35700871 266586.17111463 ... 374858.04205219\n",
      " 384150.23397343 216572.03848576]\n",
      "errors [  54800.57189742  102051.35700871   86586.17111463 ...   14858.04205219\n",
      "  -15849.76602657 -108427.96151424]\n",
      "gradient 5.092610366384292e+18\n",
      "gradient magnitude 2256681272.662201\n",
      "\n",
      " predictions [276698.8897966  640055.85057026 266572.81866948 ... 374857.23855071\n",
      " 384150.65531711 216571.5028159 ]\n",
      "errors [  54798.8897966   102055.85057026   86572.81866948 ...   14857.23855071\n",
      "  -15849.34468289 -108428.4971841 ]\n",
      "gradient 4.853819664617822e+18\n",
      "gradient magnitude 2203138594.0557218\n",
      "\n",
      " predictions [276697.24760575 640060.23751621 266559.78302842 ... 374856.45411331\n",
      " 384151.06666382 216570.97985544]\n",
      "errors [  54797.24760575  102060.23751621   86559.78302842 ...   14856.45411331\n",
      "  -15848.93333618 -108429.02014456]\n",
      "gradient 4.626225776611273e+18\n",
      "gradient magnitude 2150866285.153792\n",
      "\n",
      " predictions [276695.64437798 640064.52037616 266547.05667487 ... 374855.68828768\n",
      " 384151.46825077 216570.46930286]\n",
      "errors [  54795.64437798  102064.52037616   86547.05667487 ...   14855.68828768\n",
      "  -15848.53174923 -108429.53069714]\n",
      "gradient 4.4093036876251233e+18\n",
      "gradient magnitude 2099834204.7945411\n",
      "\n",
      " predictions [276694.07918883 640068.70161968 266534.63227056 ... 374854.94063222\n",
      " 384151.86030952 216569.97086374]\n",
      "errors [  54794.07918883  102068.70161968   86534.63227056 ...   14854.94063222\n",
      "  -15848.13969048 -108430.02913626]\n",
      "gradient 4.2025530006765056e+18\n",
      "gradient magnitude 2050012926.953512\n",
      "\n",
      " predictions [276692.55113578 640072.78365776 266522.50265137 ... 374854.21071583\n",
      " 384152.24306613 216569.48425069]\n",
      "errors [  54792.55113578  102072.78365776   86522.50265137 ...   14854.21071583\n",
      "  -15847.75693387 -108430.51574931]\n",
      "gradient 4.005496782229911e+18\n",
      "gradient magnitude 2001373723.778223\n",
      "\n",
      " predictions [276691.05933774 640076.76884418 266510.66082314 ... 374853.49811763\n",
      " 384152.61674131 216569.00918311]\n",
      "errors [  54791.05933774  102076.76884418   86510.66082314 ...   14853.49811763\n",
      "  -15847.38325869 -108430.99081689]\n",
      "gradient 3.8176804619949916e+18\n",
      "gradient magnitude 1953888549.0208983\n",
      "\n",
      " predictions [276689.6029345  640080.65947686 266499.09995765 ... 374852.80242671\n",
      " 384152.98155053 216568.54538708]\n",
      "errors [  54789.6029345   102080.65947686   86499.09995765 ...   14852.80242671\n",
      "  -15847.01844947 -108431.45461292]\n",
      "gradient 3.6386707843264435e+18\n",
      "gradient magnitude 1907530021.8676622\n",
      "\n",
      " predictions [276688.18108627 640084.45779922 266487.8133887  ... 374852.12324193\n",
      " 384153.33770414 216568.09259516]\n",
      "errors [  54788.18108627  102084.45779922   86487.8133887  ...   14852.12324193\n",
      "  -15846.66229586 -108431.90740484]\n",
      "gradient 3.46805480878777e+18\n",
      "gradient magnitude 1862271411.1503108\n",
      "\n",
      " predictions [276686.79297321 640088.16600143 266476.79460825 ... 374851.46017166\n",
      " 384153.68540751 216567.65054626]\n",
      "errors [  54786.79297321  102088.16600143   86476.79460825 ...   14851.46017166\n",
      "  -15846.31459249 -108432.34945374]\n",
      "gradient 3.3054389575798973e+18\n",
      "gradient magnitude 1818086619.9331365\n",
      "\n",
      " predictions [276685.43779489 640091.78622172 266466.03726266 ... 374850.81283357\n",
      " 384154.02486114 216567.2189855 ]\n",
      "errors [  54785.43779489  102091.78622172   86466.03726266 ...   14850.81283357\n",
      "  -15845.97513886 -108432.7810145 ]\n",
      "gradient 3.1504481076223887e+18\n",
      "gradient magnitude 1774950170.4618044\n",
      "\n",
      " predictions [276684.11476989 640095.32054757 266455.53514906 ... 374850.18085438\n",
      " 384154.35626075 216566.79766402]\n",
      "errors [  54784.11476989  102095.32054757   86455.53514906 ...   14850.18085438\n",
      "  -15845.64373925 -108433.20233598]\n",
      "gradient 3.002724725238943e+18\n",
      "gradient magnitude 1732837189.4782681\n",
      "\n",
      " predictions [276682.82313534 640098.77101693 266445.28221174 ... 374849.56386969\n",
      " 384154.67979745 216566.3863389 ]\n",
      "errors [  54782.82313534  102098.77101693   86445.28221174 ...   14849.56386969\n",
      "  -15845.32020255 -108433.6136611 ]\n",
      "gradient 2.8619280413748675e+18\n",
      "gradient magnitude 1691723393.8723161\n",
      "\n",
      " predictions [276681.56214646 640102.13961942 266435.27253866 ... 374848.96152373\n",
      " 384154.99565779 216565.98477294]\n",
      "errors [  54781.56214646  102102.13961942   86435.27253866 ...   14848.96152373\n",
      "  -15845.00434221 -108434.01522706]\n",
      "gradient 2.7277332655247585e+18\n",
      "gradient magnitude 1651585076.683838\n",
      "\n",
      " predictions [276680.33107613 640105.42829742 266425.50035807 ... 374848.37346918\n",
      " 384155.30402389 216565.59273461]\n",
      "errors [  54780.33107613  102105.42829742   86425.50035807 ...   14848.37346918\n",
      "  -15844.69597611 -108434.40726539]\n",
      "gradient 2.5998308364926807e+18\n",
      "gradient magnitude 1612399093.4296262\n",
      "\n",
      " predictions [276679.12921451 640108.63894725 266415.96003515 ... 374847.79936696\n",
      " 384155.60507358 216565.20999785]\n",
      "errors [  54779.12921451  102108.63894725   86415.96003515 ...   14847.79936696\n",
      "  -15844.39492642 -108434.79000215]\n",
      "gradient 2.477925708305176e+18\n",
      "gradient magnitude 1574142848.76093\n",
      "\n",
      " predictions [276677.95586858 640111.77342024 266406.64606877 ... 374847.23888603\n",
      " 384155.89898044 216564.83634196]\n",
      "errors [  54777.95586858  102111.77342024   86406.64606877 ...   14847.23888603\n",
      "  -15844.10101956 -108435.16365804]\n",
      "gradient 2.361736669591088e+18\n",
      "gradient magnitude 1536794283.43259\n",
      "\n",
      " predictions [276676.81036176 640114.83352377 266397.55308834 ... 374846.6917032\n",
      " 384156.18591394 216564.4715515 ]\n",
      "errors [  54776.81036176  102114.83352377   86397.55308834 ...   14846.6917032\n",
      "  -15843.81408606 -108435.5284485 ]\n",
      "gradient 2.2509956948837696e+18\n",
      "gradient magnitude 1500331861.5838861\n",
      "\n",
      " predictions [276675.69203354 640117.82102235 266388.67585068 ... 374846.15750297\n",
      " 384156.46603954 216564.11541611]\n",
      "errors [  54775.69203354  102117.82102235   86388.67585068 ...   14846.15750297\n",
      "  -15843.53396046 -108435.88458389]\n",
      "gradient 2.145447326339096e+18\n",
      "gradient magnitude 1464734558.3207545\n",
      "\n",
      " predictions [276674.60023907 640120.73763864 266380.009237   ... 374845.63597731\n",
      " 384156.73951877 216563.76773044]\n",
      "errors [  54774.60023907  102120.73763864   86380.009237   ...   14845.63597731\n",
      "  -15843.26048123 -108436.23226956]\n",
      "gradient 2.0448480844423997e+18\n",
      "gradient magnitude 1429981847.5919194\n",
      "\n",
      " predictions [276673.5343488  640123.5850544  266371.54824998 ... 374845.12682548\n",
      " 384157.00650931 216563.42829402]\n",
      "errors [  54773.5343488   102123.5850544    86371.54824998 ...   14845.12682548\n",
      "  -15842.99349069 -108436.57170598]\n",
      "gradient 1.9489659063466982e+18\n",
      "gradient magnitude 1396053690.3524513\n",
      "\n",
      " predictions [276672.49374812 640126.36491151 266363.28801087 ... 374844.62975392\n",
      " 384157.26716512 216563.09691111]\n",
      "errors [  54772.49374812  102126.36491151   86363.28801087 ...   14844.62975392\n",
      "  -15842.73283488 -108436.90308889]\n",
      "gradient 1.8575796105601224e+18\n",
      "gradient magnitude 1362930523.0128653\n",
      "\n",
      " predictions [276671.47783701 640129.07881289 266355.22375666 ... 374844.14447599\n",
      " 384157.52163651 216562.77339065]\n",
      "errors [  54771.47783701  102129.07881289   86355.22375666 ...   14844.14447599\n",
      "  -15842.47836349 -108437.22660935]\n",
      "gradient 1.7704783867056005e+18\n",
      "gradient magnitude 1330593246.1521065\n",
      "\n",
      " predictions [276670.48602967 640131.72832342 266347.35083734 ... 374843.67071189\n",
      " 384157.77007019 216562.45754607]\n",
      "errors [  54770.48602967  102131.72832342   86347.35083734 ...   14843.67071189\n",
      "  -15842.22992981 -108437.54245393]\n",
      "gradient 1.6874613092387822e+18\n",
      "gradient magnitude 1299023213.51036\n",
      "\n",
      " predictions [276669.5177542  640134.31497086 266339.66471327 ... 374843.20818842\n",
      " 384158.01260943 216562.14919527]\n",
      "errors [  54769.5177542   102134.31497086   86339.66471327 ...   14843.20818842\n",
      "  -15841.98739057 -108437.85080473]\n",
      "gradient 1.6083368739404954e+18\n",
      "gradient magnitude 1268202221.233071\n",
      "\n",
      " predictions [276668.57245229 640136.84024671 266332.16095246 ... 374842.75663889\n",
      " 384158.24939408 216561.84816044]\n",
      "errors [  54768.57245229  102136.84024671   86332.16095246 ...   14842.75663889\n",
      "  -15841.75060592 -108438.15183956]\n",
      "gradient 1.532922556160556e+18\n",
      "gradient magnitude 1238112497.3767755\n",
      "\n",
      " predictions [276667.64957886 640139.3056071  266324.83522813 ... 374842.31580293\n",
      " 384158.48056067 216561.55426801]\n",
      "errors [  54767.64957886  102139.3056071    86324.83522813 ...   14842.31580293\n",
      "  -15841.51943933 -108438.44573199]\n",
      "gradient 1.4610443897678244e+18\n",
      "gradient magnitude 1208736691.6611013\n",
      "\n",
      " predictions [276666.74860175 640141.71247359 266317.68331612 ... 374841.88542635\n",
      " 384158.7062425  216561.2673485 ]\n",
      "errors [  54766.74860175  102141.71247359   86317.68331612 ...   14841.88542635\n",
      "  -15841.2937575  -108438.7326515 ]\n",
      "gradient 1.3925365658399314e+18\n",
      "gradient magnitude 1180057865.4625084\n",
      "\n",
      " predictions [276665.86900146 640144.06223404 266310.70109251 ... 374841.46526098\n",
      " 384158.92656971 216560.98723648]\n",
      "errors [  54765.86900146  102144.06223404   86310.70109251 ...   14841.46526098\n",
      "  -15841.07343029 -108439.01276352]\n",
      "gradient 1.3272410501801283e+18\n",
      "gradient magnitude 1152059482.0494852\n",
      "\n",
      " predictions [276665.01027079 640146.35624335 266303.88453122 ... 374841.05506455\n",
      " 384159.14166933 216560.71377043]\n",
      "errors [  54765.01027079  102146.35624335   86303.88453122 ...   14841.05506455\n",
      "  -15840.85833067 -108439.28622957]\n",
      "gradient 1.265007218759258e+18\n",
      "gradient magnitude 1124725397.0455446\n",
      "\n",
      " predictions [276664.17191458 640148.5958243  266297.22970171 ... 374840.65460053\n",
      " 384159.3516654  216560.44679267]\n",
      "errors [  54764.17191458  102148.5958243    86297.22970171 ...   14840.65460053\n",
      "  -15840.6483346  -108439.55320733]\n",
      "gradient 1.2056915102600404e+18\n",
      "gradient magnitude 1098039849.12208\n",
      "\n",
      " predictions [276663.35344942 640150.78226827 266290.73276667 ... 374840.26363801\n",
      " 384159.55667901 216560.18614925]\n",
      "errors [  54763.35344942  102150.78226827   86290.73276667 ...   14840.26363801\n",
      "  -15840.44332099 -108439.81385075]\n",
      "gradient 1.1491570949021028e+18\n",
      "gradient magnitude 1071987450.907007\n",
      "\n",
      " predictions [276662.55440338 640152.916836   266284.38997985 ... 374839.88195156\n",
      " 384159.75682837 216559.93168988]\n",
      "errors [  54762.55440338  102152.916836     86284.38997985 ...   14839.88195156\n",
      "  -15840.24317163 -108440.06831012]\n",
      "gradient 1.0952735588083553e+18\n",
      "gradient magnitude 1046553180.1147782\n",
      "\n",
      " predictions [276661.77431569 640155.00075832 266278.19768389 ... 374839.50932108\n",
      " 384159.9522289  216559.68326784]\n",
      "errors [  54761.77431569  102155.00075832   86278.19768389 ...   14839.50932108\n",
      "  -15840.0477711  -108440.31673216]\n",
      "gradient 1.0439166031633569e+18\n",
      "gradient magnitude 1021722370.883283\n",
      "\n",
      " predictions [276661.01273657 640157.03523686 266272.15230819 ... 374839.14553171\n",
      " 384160.14299326 216559.44073989]\n",
      "errors [  54761.01273657  102157.03523686   86272.15230819 ...   14839.14553171\n",
      "  -15839.85700674 -108440.55926011]\n",
      "gradient 9.949677574802011e+17\n",
      "gradient magnitude 997480705.3172513\n"
     ]
    }
   ],
   "source": [
    "weights_model2 = regression_gradient_descent(feature_matrix=feature_matrix_2\n",
    "                                      , output=output_2\n",
    "                                      , initial_weights=initial_weights_2\n",
    "                                      , step_size=step_size_2\n",
    "                                      , tolerance=tolerance_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your newly estimated weights and the predict_output function to compute the predictions on the TEST data. Don't forget to create a numpy array for these features from the test set first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_feature_matrix_2, test_output_2) = get_numpy_data(test_data, model_features_2, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_2 = predict_output(feature_matrix=test_feature_matrix_2, weights=weights_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: What is the predicted price for the 1st house in the TEST data set for model 2 (round to nearest dollar)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([366651.41203656, 762662.39786164, 386312.09499712, ...,\n",
       "       682087.39928241, 585579.27865729, 216559.20396617])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the actual price for the 1st house in the test data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([310000., 650000., 233000., ..., 610685., 400000., 402101.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: Which estimate was closer to the true price for the 1st house on the TEST data set, model 1 or model 2?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your predictions and the output to compute the RSS for model 2 on TEST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270263446465244.06"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS2 = get_residual_sum_of_squares(predictions=test_predictions_2, output=test_output_2)\n",
    "RSS2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: Which model (1 or 2) has lowest RSS on all of the TEST data? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 1: 275400047593155.94 <br>\n",
    "model 2: 270263446465244.06\n",
    "\n",
    "lowest: model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
